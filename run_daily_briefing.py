# ==========================================================
# v7-Final: CrewAI Daily Briefing (ë¸Œë¦¬í•‘ ë³µì› + Notion ìë™ ì—…ë¡œë“œ)
# CrewAI 1.1.0+ í˜¸í™˜ ë²„ì „
# ==========================================================
import os, re, time, fitz, requests, pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from collections import Counter
from typing import List, Dict, Any
from dotenv import load_dotenv
from openai import OpenAI
from crewai import Agent, Task, Crew, Process
from crewai.tools import BaseTool
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# ----------------------------------------------------------
# 0ï¸âƒ£ í™˜ê²½ ì„¤ì •
# ----------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
NOTION_DATABASE_ID = os.getenv("NOTION_DATABASE_ID")
LLM_MODEL = os.getenv("OPENAI_MODEL_NAME", "gpt-5-mini")
client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"}
NOTION_HEADERS = {"Authorization": f"Bearer {NOTION_API_KEY}",
                  "Notion-Version": "2022-06-28",
                  "Content-Type": "application/json"}

# ë‚ ì§œ ì„¤ì •
from datetime import timedelta
today_display = datetime.now().strftime("%Y.%m.%d")
today_file = datetime.now().strftime("%Y-%m-%d")

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìµœê·¼ 3ì¼ì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ (ì£¼ë§ ëŒ€ì‘)
TEST_MODE_RECENT_DAYS = False  # True: ìµœê·¼ 3ì¼, False: ì˜¤ëŠ˜ë§Œ
if TEST_MODE_RECENT_DAYS:
    # 4ìë¦¬ ì—°ë„ì™€ 2ìë¦¬ ì—°ë„ ë‘˜ ë‹¤ ìƒì„±
    target_dates_full = [(datetime.now() - timedelta(days=i)).strftime("%Y.%m.%d") for i in range(3)]
    target_dates_short = [(datetime.now() - timedelta(days=i)).strftime("%y.%m.%d") for i in range(3)]
    target_dates = target_dates_full + target_dates_short  # ë‘˜ ë‹¤ í—ˆìš©
    print(f"[TEST] ìµœê·¼ 3ì¼ì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ëª¨ë“œ: {', '.join(target_dates_full)}")
else:
    target_dates = [today_display, datetime.now().strftime("%y.%m.%d")]
    print(f"[PROD] ì˜¤ëŠ˜ ë‚ ì§œë§Œ ìˆ˜ì§‘: {today_display}")

# ----------------------------------------------------------
# ğŸŒ Selenium í—¬í¼ í•¨ìˆ˜
# ----------------------------------------------------------
def create_selenium_driver():
    """Selenium Chrome ë“œë¼ì´ë²„ ìƒì„± (headless ëª¨ë“œ)"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument(f'user-agent={HEADERS["User-Agent"]}')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

# ----------------------------------------------------------
# 1ï¸âƒ£ ë„¤ì´ë²„ / í•œê²½ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ Tool
# ----------------------------------------------------------
class NaverResearchScraperTool(BaseTool):
    name: str = "Naver Research Scraper"
    description: str = "ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"
    
    def _run(self) -> str:
        """ë„¤ì´ë²„ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ (Selenium ì‚¬ìš©)"""
        base_url = "https://finance.naver.com/research/"
        categories = {"íˆ¬ìì •ë³´": "invest_list.naver",
                      "ì¢…ëª©ë¶„ì„": "company_list.naver",
                      "ì‚°ì—…ë¶„ì„": "industry_list.naver",
                      "ê²½ì œë¶„ì„": "economy_list.naver"}
        reports = []
        
        print(f"\n[DEBUG] ë„¤ì´ë²„ ìˆ˜ì§‘ ì‹œì‘ (Selenium) - ê²€ìƒ‰ ë‚ ì§œ: {target_dates}")
        
        driver = None
        try:
            driver = create_selenium_driver()
            
            for cat, path in categories.items():
                try:
                    url = base_url + path
                    driver.get(url)
                    
                    # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                    time.sleep(2)
                    
                    # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
                    soup = BeautifulSoup(driver.page_source, "html.parser")
                    rows = soup.select("table.type_1 tbody tr")
                    print(f"[DEBUG] {cat}: {len(rows)}ê°œ row ë°œê²¬")
                    
                    row_count = 0
                    collected_in_category = 0
                    for row in rows:
                        cols = row.find_all("td")
                        if len(cols) < 4: 
                            continue
                        
                        # ì»¬ëŸ¼ ìˆœì„œ: [ì œëª©, ì¦ê¶Œì‚¬, ì œê³µì¼ì, PDF] ë˜ëŠ” ë‹¤ë¥¸ êµ¬ì¡°ì¼ ìˆ˜ ìˆìŒ
                        title_tag = cols[0].find("a")
                        if not title_tag:
                            title_tag = cols[1].find("a")
                        
                        company = cols[1].get_text(strip=True) if len(cols) > 1 else "N/A"
                        date = cols[-1].get_text(strip=True)  # ë§ˆì§€ë§‰ ì»¬ëŸ¼ì´ ë‚ ì§œì¼ ê°€ëŠ¥ì„±
                        if not date or len(date) < 6:  # ë‚ ì§œê°€ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´
                            date = cols[-2].get_text(strip=True) if len(cols) > 2 else ""
                        
                        if row_count < 3:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                            title_text = title_tag.get_text(strip=True)[:30] if title_tag else 'N/A'
                            print(f"   - [{date}] {title_text}... (ì»¬ëŸ¼ìˆ˜: {len(cols)})")
                        row_count += 1
                        
                        # ë‚ ì§œ í•„í„°: target_dates ëª©ë¡ì— ìˆëŠ” ë‚ ì§œë§Œ ìˆ˜ì§‘
                        if date not in target_dates:
                            continue
                        
                        if not title_tag: continue
                        detail_url = "https://finance.naver.com" + title_tag.get("href", "")
                        pdf_url = None
                        try:
                            d_soup = BeautifulSoup(requests.get(detail_url, headers=HEADERS).text, "html.parser")
                            pdf_btn = d_soup.find("a", string=re.compile("ë¦¬í¬íŠ¸ë³´ê¸°"))
                            if pdf_btn: pdf_url = "https://finance.naver.com" + pdf_btn["href"]
                        except: pass
                        reports.append({"source": "ë„¤ì´ë²„", "category": cat, "title": title_tag.get_text(strip=True),
                                        "company": company, "date": date, "url": detail_url, "pdf_url": pdf_url})
                        collected_in_category += 1
                    
                    print(f"   [OK] {cat}: {collected_in_category}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                    time.sleep(1)
                except Exception as e:
                    print(f"âš ï¸ ë„¤ì´ë²„ {cat} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        finally:
            if driver:
                driver.quit()
        
        return str(reports)

class HankyungScraperTool(BaseTool):
    name: str = "Hankyung Consensus Scraper"
    description: str = "í•œê²½ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"
    
    def _run(self) -> str:
        """í•œê²½ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"""
        url = "https://consensus.hankyung.com/analysis/list"
        reports = []
        try:
            print(f"\n[DEBUG] í•œê²½ ìˆ˜ì§‘ ì‹œì‘ - ê²€ìƒ‰ ë‚ ì§œ: {target_dates[:3]}")
            res = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")
            rows = soup.select("table tbody tr")
            print(f"[DEBUG] í•œê²½: {len(rows)}ê°œ row ë°œê²¬")
            
            row_count = 0
            collected_count = 0
            for row in rows:
                cols = row.find_all("td")
                if len(cols) < 4: 
                    continue
                    
                date_raw = cols[3].get_text(strip=True)
                # ë‚ ì§œ í˜•ì‹ í†µì¼ (YYYY-MM-DD â†’ YYYY.MM.DD, YY-MM-DD â†’ YY.MM.DD)
                date = date_raw.replace("-", ".")
                
                title_tag = cols[0].find("a")
                title_text = title_tag.get_text(strip=True)[:30] if title_tag else 'N/A'
                
                if row_count < 3:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                    print(f"   - [{date}] {title_text}... (ì»¬ëŸ¼ìˆ˜: {len(cols)})")
                row_count += 1
                
                # ë‚ ì§œ í•„í„°: target_dates ëª©ë¡ì— ìˆëŠ” ë‚ ì§œë§Œ ìˆ˜ì§‘
                if date not in target_dates:
                    continue
                
                if not title_tag:
                    continue
                    
                pdf_tag = row.find("a", href=re.compile(r"\.pdf$"))
                pdf_url = "https://consensus.hankyung.com" + pdf_tag["href"] if pdf_tag else None
                reports.append({"source": "í•œê²½ì»¨ì„¼ì„œìŠ¤", "category": cols[2].get_text(strip=True),
                                "title": title_tag.get_text(strip=True), "company": cols[1].get_text(strip=True),
                                "date": date, "url": "https://consensus.hankyung.com" + title_tag["href"],
                                "pdf_url": pdf_url})
                collected_count += 1
                time.sleep(0.5)
            
            print(f"   [OK] í•œê²½: {collected_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ í•œê²½ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        return str(reports)

# ----------------------------------------------------------
# 2ï¸âƒ£ Python Analyzer Tool
# ----------------------------------------------------------
class PythonAnalyzerTool(BaseTool):
    name: str = "Python Analyzer Tool"
    description: str = "ë¦¬í¬íŠ¸ ì œëª© ê¸°ë°˜ í‚¤ì›Œë“œ/ì¹´í…Œê³ ë¦¬ ë¶„ì„"
    
    def _run(self, reports_str: str) -> str:
        """í‚¤ì›Œë“œ ë° ì¹´í…Œê³ ë¦¬ ë¶„ì„"""
        try:
            reports = eval(reports_str)
            df = pd.DataFrame(reports).drop_duplicates(subset=["title", "company"])
            words = sum([re.findall(r"[ê°€-í£A-Za-z0-9]{2,12}", t) for t in df["title"]], [])
            stop = {"ë¦¬í¬íŠ¸", "ë¶„ì„", "ì „ë§", "íˆ¬ì", "ê²½ì œ", "ì‚°ì—…", "ì´ìŠˆ"}
            counter = Counter([w for w in words if w not in stop])
            result = {
                "top_keywords": ", ".join([f"{k}({v}íšŒ)" for k, v in counter.most_common(7)]),
                "category_summary": df["category"].value_counts().to_dict()
            }
            return str(result)
        except Exception as e:
            return f"ë¶„ì„ ì‹¤íŒ¨: {e}"

# ----------------------------------------------------------
# 3ï¸âƒ£ Report Summarizer Tool
# ----------------------------------------------------------
class ReportSummarizerTool(BaseTool):
    name: str = "Report Summarizer Tool"
    description: str = "PDF ë¦¬í¬íŠ¸ 2~3ì¤„ ìš”ì•½"
    
    def _run(self, reports_str: str) -> str:
        """ë¦¬í¬íŠ¸ ìš”ì•½"""
        try:
            reports = eval(reports_str)
            summaries = []
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìµœëŒ€ 3ê°œë§Œ ìš”ì•½ (ì‹œê°„ ë‹¨ì¶•)
            max_reports = 3 if TEST_MODE_RECENT_DAYS else 5
            for r in reports[:max_reports]:
                title, company, category, pdf_url = r["title"], r["company"], r["category"], r.get("pdf_url")
                text = ""
                if pdf_url:
                    try:
                        res = requests.get(pdf_url, headers=HEADERS, timeout=15)
                        with open("temp.pdf", "wb") as f: f.write(res.content)
                        with fitz.open("temp.pdf") as pdf:
                            for page in pdf: text += page.get_text()
                        os.remove("temp.pdf")
                    except Exception as e: 
                        text = f"[PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}]"
                
                prompt = f"""
ë‹¹ì‹ ì€ 20ë…„ì°¨ ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
{company}ì˜ '{title}' ë¦¬í¬íŠ¸ë¥¼ 2~3ì¤„ë¡œ ìš”ì•½í•˜ì„¸ìš”.
ì‚¬ì‹¤ë§Œ ë‚¨ê¸°ê³  ì „ë§/ì˜ˆì¸¡ì€ ì œì™¸í•©ë‹ˆë‹¤.
ë³¸ë¬¸:
{text[:5000]}
"""
                try:
                    resp = client.chat.completions.create(
                        model=LLM_MODEL,
                        messages=[{"role": "system", "content": "ì‚¬ì‹¤ ê¸°ë°˜ ìš”ì•½ë§Œ ìˆ˜í–‰."},
                                  {"role": "user", "content": prompt}])
                    summary = resp.choices[0].message.content.strip()
                except Exception as e:
                    summary = f"[ìš”ì•½ ì‹¤íŒ¨: {e}]"
                
                summaries.append({"category": category, "title": title,
                                  "company": company, "summary": summary})
                time.sleep(1)
            return str(summaries)
        except Exception as e:
            return f"ìš”ì•½ ì‹¤íŒ¨: {e}"

# ----------------------------------------------------------
# 4ï¸âƒ£ Final Briefing Tool
# ----------------------------------------------------------
class FinalBriefingTool(BaseTool):
    name: str = "Final Briefing Tool"
    description: str = "ë¶„ì„ ê²°ê³¼ì™€ ìš”ì•½ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¢…í•©í•´ ì¼ì¼ ë¸Œë¦¬í•‘ í…ìŠ¤íŠ¸ ì‘ì„±"
    
    def _run(self, summaries_str: str, analysis_str: str) -> str:
        """ìµœì¢… ë¸Œë¦¬í•‘ ìƒì„±"""
        try:
            summaries = eval(summaries_str)
            analysis = eval(analysis_str)
            
            summary_texts = [f"[{s['category']}] {s['title']} â€” {s['summary']} ({s['company']})"
                             for s in summaries]
            prompt = f"""
[ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ]
{analysis.get("top_keywords", "N/A")}
[ì¹´í…Œê³ ë¦¬ë³„ ë¹„ì¤‘]
{analysis.get("category_summary", {})}

[ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸]
{chr(10).join(summary_texts)}

---
ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ íˆ¬ì ìŠ¤í„°ë””ìš© 'ì¼ì¼ ë¸Œë¦¬í•‘'ì„ ì‘ì„±í•˜ì‹œì˜¤.
1) í•µì‹¬ í…Œë§ˆ TOP 3~5
2) ê±°ì‹œê²½ì œ ìš”ì•½
3) ì£¼ìš” ì¢…ëª© ë° ì‚°ì—…ë³„ ìš”ì•½
"""
            resp = client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "system", "content": "20ë…„ì°¨ ë¦¬ì„œì¹˜ ì• ë„ë¦¬ìŠ¤íŠ¸ë¡œ ì‚¬ì‹¤ ê¸°ë°˜ ë¸Œë¦¬í•‘ ì‘ì„±"},
                          {"role": "user", "content": prompt}])
            body = resp.choices[0].message.content.strip()
            header = f"# {today_file} ì¼ì¼ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë¸Œë¦¬í•‘\n\n*ì´ {len(summaries)}ê±´ ê¸°ë°˜ / {today_display} ë°œí–‰*\n\n"
            return header + body
        except Exception as e:
            return f"ë¸Œë¦¬í•‘ ìƒì„± ì‹¤íŒ¨: {e}"

# ----------------------------------------------------------
# 5ï¸âƒ£ Notion Upload Tool
# ----------------------------------------------------------
class NotionUploadTool(BaseTool):
    name: str = "Notion Upload Tool"
    description: str = "ìµœì¢… ë¸Œë¦¬í•‘ê³¼ ë¶„ì„ê²°ê³¼ë¥¼ Notion DBì— ì—…ë¡œë“œ"
    
    def _run(self, briefing_text: str, analysis_str: str) -> str:
        """Notionì— ì—…ë¡œë“œ"""
        try:
            analysis = eval(analysis_str)
            page_data = {
                "parent": {"database_id": NOTION_DATABASE_ID},
                "properties": {
                    "Date": {"date": {"start": today_file}},
                    "Top Keywords": {"rich_text": [{"text": {"content": analysis.get("top_keywords", "")[:2000]}}]},
                    "Category Summary": {"rich_text": [{"text": {"content": str(analysis.get("category_summary", {}))[:2000]}}]},
                    "Name": {"title": [{"text": {"content": f"{today_file} ì¼ì¼ ë¸Œë¦¬í•‘"}}]}
                }
            }
            res = requests.post("https://api.notion.com/v1/pages", headers=NOTION_HEADERS, json=page_data)
            res.raise_for_status()
            parent_id = res.json()["id"]
            
            children = []
            for i in range(0, len(briefing_text), 2000):
                children.append({"object": "block", "type": "paragraph",
                                 "paragraph": {"rich_text": [{"type": "text", "text": {"content": briefing_text[i:i+2000]}}]}})
            
            res_blocks = requests.patch(f"https://api.notion.com/v1/blocks/{parent_id}/children",
                                        headers=NOTION_HEADERS, json={"children": children})
            res_blocks.raise_for_status()
            return f"âœ… Notion ì—…ë¡œë“œ ì™„ë£Œ (Page ID: {parent_id})"
        except Exception as e:
            return f"âš ï¸ Notion ì—…ë¡œë“œ ì‹¤íŒ¨: {e}"

# ----------------------------------------------------------
# 6ï¸âƒ£ ê°„ì†Œí™”ëœ ì‹¤í–‰ í•¨ìˆ˜
# ----------------------------------------------------------
def run_daily_briefing():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print(f"[START] {today_display} Daily Briefing ì‹œì‘ (v7-Final)")
    
    # 1. ë¦¬í¬íŠ¸ ìˆ˜ì§‘
    print("\n[1/5] ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì¤‘...")
    naver_tool = NaverResearchScraperTool()
    hankyung_tool = HankyungScraperTool()
    naver_reports = eval(naver_tool._run())
    hankyung_reports = eval(hankyung_tool._run())
    all_reports = naver_reports + hankyung_reports
    print(f"   [OK] ì´ {len(all_reports)}ê°œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"   (ë„¤ì´ë²„: {len(naver_reports)}ê°œ, í•œê²½: {len(hankyung_reports)}ê°œ)")
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìˆ˜ì§‘ëœ ë¦¬í¬íŠ¸ ìƒ˜í”Œ ì¶œë ¥
    if TEST_MODE_RECENT_DAYS and len(all_reports) > 0:
        print(f"\n   [DEBUG] ìˆ˜ì§‘ ìƒ˜í”Œ:")
        for i, r in enumerate(all_reports[:3], 1):
            print(f"   {i}. [{r['date']}] {r['title'][:30]}... ({r['company']})")
    
    # ë¦¬í¬íŠ¸ê°€ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
    if len(all_reports) == 0:
        print("\n" + "="*60)
        print("[INFO] ìˆ˜ì§‘ëœ ë¦¬í¬íŠ¸ê°€ ì—†ì–´ ë¸Œë¦¬í•‘ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("ì£¼ë§ì´ë‚˜ ê³µíœ´ì¼ì—ëŠ” ë¦¬í¬íŠ¸ê°€ ë°œí–‰ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("="*60)
        return "[INFO] ë¦¬í¬íŠ¸ ì—†ìŒ - ë¸Œë¦¬í•‘ ë¯¸ìƒì„±"
    
    # 2. í‚¤ì›Œë“œ ë¶„ì„
    print("\n[2/5] í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
    analyzer = PythonAnalyzerTool()
    analysis = eval(analyzer._run(str(all_reports)))
    print(f"   [OK] í‚¤ì›Œë“œ: {analysis['top_keywords']}")
    
    # 3. ë¦¬í¬íŠ¸ ìš”ì•½
    print("\n[3/5] ë¦¬í¬íŠ¸ ìš”ì•½ ì¤‘...")
    summarizer = ReportSummarizerTool()
    summaries = eval(summarizer._run(str(all_reports)))
    print(f"   [OK] {len(summaries)}ê°œ ë¦¬í¬íŠ¸ ìš”ì•½ ì™„ë£Œ")
    
    # 4. ìµœì¢… ë¸Œë¦¬í•‘ ìƒì„±
    print("\n[4/5] ìµœì¢… ë¸Œë¦¬í•‘ ìƒì„± ì¤‘...")
    briefing_tool = FinalBriefingTool()
    briefing = briefing_tool._run(str(summaries), str(analysis))
    print(f"   [OK] ë¸Œë¦¬í•‘ ìƒì„± ì™„ë£Œ ({len(briefing)} ì)")
    
    # 5. Notion ì—…ë¡œë“œ
    print("\n[5/5] Notion ì—…ë¡œë“œ ì¤‘...")
    notion_tool = NotionUploadTool()
    result = notion_tool._run(briefing, str(analysis))
    # Windows ì¸ì½”ë”© ì—ëŸ¬ ë°©ì§€ (ì´ëª¨ì§€ ì œê±°)
    try:
        result_str = str(result).encode('ascii', 'ignore').decode('ascii')
        print(f"   {result_str}")
    except:
        print("   [OK] Notion ì—…ë¡œë“œ ì™„ë£Œ (ê²°ê³¼ ì¶œë ¥ ìƒëµ)")
    
    print("\n[COMPLETE] ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    return briefing

if __name__ == "__main__":
    # ----------------------------------------------------------
    # ì£¼ë§ ìë™ ì‹¤í–‰ ë°©ì§€ (í† ìš”ì¼=5, ì¼ìš”ì¼=6)
    # ----------------------------------------------------------
    weekday = datetime.today().weekday()  # ì›”=0, í™”=1, ..., ì¼=6
    IS_TEST_MODE = False  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì˜¤ëŠ˜ë§Œ ê°•ì œ ì‹¤í–‰í•˜ë ¤ë©´ True
    
    if weekday >= 5 and not IS_TEST_MODE:
        print("="*60)
        print("[SKIP] ì£¼ë§ì—ëŠ” ë¦¬í¬íŠ¸ê°€ ë°œí–‰ë˜ì§€ ì•Šì•„ ì‹¤í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print(f"ì˜¤ëŠ˜: {today_display} ({['ì›”','í™”','ìˆ˜','ëª©','ê¸ˆ','í† ','ì¼'][weekday]}ìš”ì¼)")
        print("ë‹¤ìŒ ì‹¤í–‰: ì›”ìš”ì¼ ì˜¤ì „ 7ì‹œ (KST)")
        print("="*60)
    else:
        if IS_TEST_MODE and weekday >= 5:
            print("="*60)
            print(f"[TEST MODE] ì£¼ë§ì´ì§€ë§Œ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print(f"ì˜¤ëŠ˜: {today_display} ({['ì›”','í™”','ìˆ˜','ëª©','ê¸ˆ','í† ','ì¼'][weekday]}ìš”ì¼)")
            print("="*60)
        
        try:
            result = run_daily_briefing()
            print("\n" + "="*60)
            print("ìµœì¢… ë¸Œë¦¬í•‘ ë¯¸ë¦¬ë³´ê¸°:")
            print("="*60)
            # Windows ì¸ì½”ë”© ì—ëŸ¬ ë°©ì§€
            try:
                result_preview = result[:500] + "..." if len(result) > 500 else result
                # ASCIIë¡œ ë³€í™˜ (ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°)
                print(result_preview.encode('ascii', 'ignore').decode('ascii'))
            except:
                print(f"   [OK] ë¸Œë¦¬í•‘ ìƒì„± ì™„ë£Œ ({len(result)}ì) - Notion í™•ì¸ í•„ìš”")
        except Exception as e:
            print(f"\n[ERROR] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
