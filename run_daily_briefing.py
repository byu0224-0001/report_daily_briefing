# ==========================================================
# CrewAI Daily Briefing v11.2 (í†µí•© ê°œì„  ì•ˆì •í™” ë²„ì „ - ì‚¬ì‹¤ ê¸°ë°˜ ì •ë¦¬ ê°•í™”)
# Phase 1 (ê¸´ê¸‰): PDF í•„í„° ê°•í™”, ì‹ í•œíˆ¬ì HTML ê²€ì¦, ì¸ì½”ë”© ìˆ˜ì •
# Phase 2 (êµ¬ì¡°): Mobile UA ì „ì—­í™”, meta refresh ì¶”ì (ì¬ì‹œë„ ì œí•œ), iframe JS ì²˜ë¦¬
# Phase 3 (ìµœì í™”): PDF ìºì‹±, ì¤‘ë³µ ì œê±°, ë¡œê¹… ê°œì„ 
# v11.1: PDF URL whitelist ê²€ì¦, HTML fallback ê°•í™”, ì¸ì½”ë”© ìˆœì„œ ìˆ˜ì •
# v11.2: LLM ì¶”ë¡  ìµœì†Œí™”, ë¦¬í¬íŠ¸ ì›ë¬¸ ì •ë³´ ì¤‘ì‹¬ ì •ë¦¬ë¡œ ë³€ê²½
# ==========================================================
import sys
import os  # ì¸ì½”ë”© ì„¤ì • ì „ì— ë¨¼ì € import
import hashlib  # Phase 3: PDF ìºì‹±ìš©
import logging  # Phase 3: ë¡œê¹… ê°œì„ ìš©
import json  # Phase 3: ìºì‹œ ì €ì¥ìš©

# Windows Unicode ì¸ì½”ë”© ê°•ì œ ì„¤ì • (Phase 1)
if sys.platform == "win32":
    os.environ["PYTHONIOENCODING"] = "utf-8"
    os.environ["PYTHONUTF8"] = "1"  # ì¶”ê°€ UTF-8 ê°•ì œ
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')

import re, time, fitz, requests, pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from collections import Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
from openai import OpenAI
from crewai.tools import BaseTool
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import urljoin  # v10.4: URL ì •ê·œí™”

# ----------------------------------------------------------
# 0ï¸âƒ£ í™˜ê²½ ì„¤ì •
# ----------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
NOTION_DATABASE_ID = os.getenv("NOTION_DATABASE_ID")

# í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸: gpt-4o-mini (ì••ì¶•) + gpt-5-mini (ë¸Œë¦¬í•‘)
LLM_SUMMARY = "gpt-4o-mini"  # ë¦¬í¬íŠ¸ ìš”ì•½ìš© (ë¹ ë¥´ê³  ì €ë ´)
LLM_BRIEFING = os.getenv("OPENAI_MODEL_NAME", "gpt-5-mini")  # ë¸Œë¦¬í•‘ ìƒì„±ìš©
client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"}
NOTION_HEADERS = {"Authorization": f"Bearer {NOTION_API_KEY}",
                  "Notion-Version": "2022-06-28",
                  "Content-Type": "application/json"}

today_display = datetime.now().strftime("%Y.%m.%d")
today_file = datetime.now().strftime("%Y-%m-%d")

# í”„ë¡œë•ì…˜ ëª¨ë“œ: ì˜¤ëŠ˜ ë‚ ì§œë§Œ ìˆ˜ì§‘
TEST_MODE_RECENT_DAYS = False  # True: ìµœê·¼ 3ì¼ (í…ŒìŠ¤íŠ¸), False: ì˜¤ëŠ˜ë§Œ (í”„ë¡œë•ì…˜)
if TEST_MODE_RECENT_DAYS:
    # 4ìë¦¬ ì—°ë„ì™€ 2ìë¦¬ ì—°ë„ ë‘˜ ë‹¤ ìƒì„±
    target_dates_full = [(datetime.now() - timedelta(days=i)).strftime("%Y.%m.%d") for i in range(3)]
    target_dates_short = [(datetime.now() - timedelta(days=i)).strftime("%y.%m.%d") for i in range(3)]
    target_dates = target_dates_full + target_dates_short  # ë‘˜ ë‹¤ í—ˆìš©
    print(f"[TEST] ìµœê·¼ 3ì¼ì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ëª¨ë“œ: {', '.join(target_dates_full)}")
else:
    target_dates = [today_display, datetime.now().strftime("%y.%m.%d")]
    print(f"[PROD] ì˜¤ëŠ˜ ë‚ ì§œë§Œ ìˆ˜ì§‘: {today_display}")

# ----------------------------------------------------------
# ğŸŒ Selenium ì„¤ì • (Phase 2: Mobile UA ì „ì—­ ì ìš©)
# ----------------------------------------------------------
# Phase 2: Mobile User-Agent ì „ì—­ ì ìš© (ì‹ í•œíˆ¬ì JS í˜ì´ì§€ ëŒ€ì‘)
MOBILE_USER_AGENT = ("Mozilla/5.0 (Linux; Android 10; SM-G973F) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/124.0.0.0 Mobile Safari/537.36")

def create_selenium_driver(force_mobile=False):
    """Selenium ë“œë¼ì´ë²„ ìƒì„± (Phase 2: Mobile UA ì˜µì…˜)"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    
    # Phase 2: Mobile UA ì¡°ê±´ë¶€ ì ìš©
    if force_mobile:
        chrome_options.add_argument(f"user-agent={MOBILE_USER_AGENT}")
        print(f"      [DEBUG] Mobile User-Agent ì ìš©")
    else:
        chrome_options.add_argument(f"user-agent={HEADERS['User-Agent']}")
    
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=chrome_options)

# ----------------------------------------------------------
# 1ï¸âƒ£ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
# ----------------------------------------------------------
class NaverResearchScraperTool(BaseTool):
    name: str = "Naver Research Scraper Tool"
    description: str = "ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"
    
    def _run(self) -> str:
        """ë„¤ì´ë²„ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ (Selenium)"""
        base_url = "https://finance.naver.com/research/"
        categories = {
            "íˆ¬ìì •ë³´": "invest_list.naver",
            "ì¢…ëª©ë¶„ì„": "company_list.naver",
            "ì‚°ì—…ë¶„ì„": "industry_list.naver",
            "ê²½ì œë¶„ì„": "economy_list.naver"
        }
        reports = []
        driver = create_selenium_driver()
        print(f"\n[DEBUG] ë„¤ì´ë²„ ìˆ˜ì§‘ ì‹œì‘ - ë‚ ì§œ: {target_dates}")
        try:
            for cat, path in categories.items():
                url = base_url + path
                print(f"\n[DEBUG] {cat} í˜ì´ì§€ ì ‘ì†: {url}")
                driver.get(url)
                time.sleep(3)  # ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                
                # í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥ (ë””ë²„ê¹…ìš©)
                page_source = driver.page_source
                if "table" not in page_source.lower():
                    print(f"   âš ï¸ {cat}: í˜ì´ì§€ ì†ŒìŠ¤ì— 'table' ì—†ìŒ")
                    continue
                
                soup = BeautifulSoup(page_source, "html.parser")
                
                # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
                rows = soup.select("table.type_1 tbody tr")
                if not rows:
                    rows = soup.select("table tbody tr")
                if not rows:
                    rows = soup.select("tbody tr")
                if not rows:
                    rows = soup.find_all("tr")
                
                print(f"[DEBUG] {cat}: {len(rows)}ê°œ row ë°œê²¬")
                for i, row in enumerate(rows):
                    cols = row.find_all("td")
                    if len(cols) < 4: 
                        continue
                    
                    # ì»¬ëŸ¼ êµ¬ì¡° ë¶„ì„ (ì¢…ëª©ëª…, ì œëª©, ì¦ê¶Œì‚¬, ì²¨ë¶€, ì‘ì„±ì¼, ì¡°íšŒìˆ˜)
                    # ì œëª©ì€ ë³´í†µ cols[0] ë˜ëŠ” cols[1]
                    title_tag = cols[0].find("a")
                    if not title_tag and len(cols) > 1:
                        title_tag = cols[1].find("a")
                    
                    # ì¦ê¶Œì‚¬ëŠ” ë³´í†µ cols[1] ë˜ëŠ” cols[2]
                    company = cols[2].get_text(strip=True) if len(cols) > 2 else "N/A"
                    if not company or company == "":
                        company = cols[1].get_text(strip=True) if len(cols) > 1 else "N/A"
                    
                    # ë‚ ì§œ ì°¾ê¸°: ë’¤ì—ì„œ ë‘ ë²ˆì§¸ ì»¬ëŸ¼ (ì‘ì„±ì¼)
                    date = ""
                    if len(cols) >= 6:  # 6ê°œ ì»¬ëŸ¼: [ì¢…ëª©ëª…, ì œëª©, ì¦ê¶Œì‚¬, ì²¨ë¶€, ì‘ì„±ì¼, ì¡°íšŒìˆ˜]
                        date = cols[4].get_text(strip=True)  # ì‘ì„±ì¼ (5ë²ˆì§¸, 0-indexed)
                    elif len(cols) >= 5:  # 5ê°œ ì»¬ëŸ¼: [ì œëª©, ì¦ê¶Œì‚¬, ì²¨ë¶€, ì‘ì„±ì¼, ì¡°íšŒìˆ˜]
                        date = cols[3].get_text(strip=True)  # ì‘ì„±ì¼
                    else:
                        date = cols[-2].get_text(strip=True)  # ë’¤ì—ì„œ ë‘ ë²ˆì§¸
                    
                    # ë””ë²„ê·¸: ì²˜ìŒ 5ê°œ row ì¶œë ¥
                    if i < 5:
                        title_text = title_tag.get_text(strip=True)[:30] if title_tag else 'N/A'
                        print(f"   - [{date}] {title_text}... (ì»¬ëŸ¼ìˆ˜: {len(cols)})")
                    
                    # ë‚ ì§œ í˜•ì‹ í†µì¼ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
                    date_clean = date.replace(" ", "").replace(".", ".").strip()
                    
                    # ë‚ ì§œ í•„í„°
                    if date_clean not in target_dates:
                        continue
                    if not title_tag:
                        continue
                    
                    # href ì¶”ì¶œ ë° ê²€ì¦
                    href = title_tag.get("href", "")
                    if not href or href == "#":
                        continue
                    
                    # v10.7: ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ (ê¸ˆì§€ëœ íŒ¨í„´ë§Œ ì°¨ë‹¨)
                    # ì¢…ëª©ë¶„ì„ì€ /item/ í—ˆìš© (ì¢…ëª© í˜ì´ì§€ë¡œ ë§í¬ê°€ ê°€ë”ë¼ë„ PDFëŠ” ì²¨ë¶€ ì»¬ëŸ¼ì— ìˆìŒ)
                    excluded_patterns = ["/chart/", "/quote/", "/news/"]  # /item/, /frgn/ ì œê±°
                    if cat != "ì¢…ëª©ë¶„ì„":  # ì¢…ëª©ë¶„ì„ì´ ì•„ë‹ˆë©´ /item/ë„ ì°¨ë‹¨
                        excluded_patterns.append("/item/")
                        excluded_patterns.append("/frgn/")

                    if any(pattern in href for pattern in excluded_patterns):
                        # ì¢…ëª©/ì°¨íŠ¸ í˜ì´ì§€ëŠ” ìŠ¤í‚µí•˜ë˜ ë¡œê·¸ ì¶œë ¥
                        if i < 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê·¸ ì¶œë ¥
                            print(f"      [DEBUG] ê¸ˆì§€ëœ URL íŒ¨í„´ ê°ì§€, ìŠ¤í‚µ: {href[:60]}...")
                        continue
                    
                    # v10.8: ì¢…ëª©ë¶„ì„ ì¹´í…Œê³ ë¦¬ í•„í„° ì œê±°
                    # (ì¢…ëª©ë¶„ì„ì€ /item/ ë§í¬ë¥¼ í—ˆìš©í•˜ê³ , PDFëŠ” ì²¨ë¶€ ì»¬ëŸ¼ì—ì„œ ì§ì ‘ ì°¾ìŒ)

                    # v10.4: URL ì •ê·œí™” (urljoinìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ê°•ì œ ë³€í™˜)
                    detail_url = urljoin("https://finance.naver.com", href)
                    
                    # PDF URL ì¶”ì¶œ: ëª©ë¡ì—ì„œ ì§ì ‘ ì°¾ê¸° (V9.3 ë°©ì‹)
                    pdf_url = None
                    try:
                        # ëª¨ë“  ì»¬ëŸ¼ ìˆœíšŒí•˜ë©° PDF ë§í¬ ì°¾ê¸°
                        for col_idx, col in enumerate(cols):
                            # 1. <a> íƒœê·¸ì—ì„œ href ì°¾ê¸°
                            pdf_link = col.find("a", href=re.compile(r"\.pdf|download|filekey|attach|report|view", re.IGNORECASE))
                            if pdf_link:
                                href = pdf_link.get("href", "")
                                if href:
                                    pdf_url = urljoin("https://finance.naver.com", href)
                                    print(f"      [DEBUG PDF] ì²¨ë¶€ ë§í¬ ë°œê²¬ (ì»¬ëŸ¼ {col_idx})")
                                    print(f"      [DEBUG PDF] ëª©ë¡ì—ì„œ PDF ë§í¬ ë°œê²¬: {pdf_url[:80]}...")
                                    break
                            
                            # 2. ì´ë¯¸ì§€ alt/titleì—ì„œ PDF í™•ì¸
                            img = col.find("img")
                            if img and ("pdf" in (img.get("alt", "") + img.get("title", "")).lower()):
                                # ë¶€ëª¨ <a> ì°¾ê¸°
                                parent_a = col.find("a")
                                if parent_a:
                                    href = parent_a.get("href", "")
                                    if href:
                                        pdf_url = urljoin("https://finance.naver.com", href)
                                        print(f"      [DEBUG PDF] ì²¨ë¶€ ì´ë¯¸ì§€ ë°œê²¬ (ì»¬ëŸ¼ {col_idx})")
                                        print(f"      [DEBUG PDF] ëª©ë¡ì—ì„œ PDF ë§í¬ ë°œê²¬: {pdf_url[:80]}...")
                                        break
                            
                            # 3. svg ì•„ì´ì½˜ í™•ì¸
                            svg = col.find("svg")
                            if svg:
                                parent_a = col.find("a")
                                if parent_a:
                                    href = parent_a.get("href", "")
                                    if href and (".pdf" in href.lower() or "download" in href.lower() or "filekey" in href.lower()):
                                        pdf_url = urljoin("https://finance.naver.com", href)
                                        print(f"      [DEBUG PDF] ì²¨ë¶€ ì•„ì´ì½˜ ë°œê²¬ (ì»¬ëŸ¼ {col_idx})")
                                        print(f"      [DEBUG PDF] ëª©ë¡ì—ì„œ PDF ë§í¬ ë°œê²¬: {pdf_url[:80]}...")
                                        break
                        
                        # ì‹ í•œíˆ¬ìì¦ê¶Œ ë¦¬í¬íŠ¸ ì²´í¬: PDFê°€ ì—†ìœ¼ë©´ ìƒì„¸ í˜ì´ì§€ ë³¸ë¬¸ë§Œ ì‚¬ìš©
                        if not pdf_url and "ì‹ í•œ" in company:
                            # v10.7: URL ìœ íš¨ì„± ì²´í¬ í›„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ (ìŠ¤í‚µ ì œê±°)
                            if not detail_url or ("read.naver" not in detail_url and "/research/" not in detail_url):
                                print(f"      [INFO] ì‹ í•œíˆ¬ìì¦ê¶Œ ë¦¬í¬íŠ¸: URL ìœ íš¨í•˜ì§€ ì•ŠìŒ (PDF/HTML ëª¨ë‘ ì‹œë„)")
                            else:
                                print(f"      [INFO] ì‹ í•œíˆ¬ìì¦ê¶Œ ë¦¬í¬íŠ¸: ìƒì„¸ í˜ì´ì§€ ë³¸ë¬¸ë§Œ ì‚¬ìš© (PDF URL ì—†ìŒ)")
                            print(f"      [WARN] PDF URL ì—†ìŒ: {title_tag.get_text(strip=True)[:30]}...")
                        
                        # PDFê°€ ì—†ëŠ” ê²½ìš° ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì‹œë„
                        if not pdf_url:
                            try:
                                d_res = requests.get(detail_url, headers=HEADERS, timeout=5)
                                d_soup = BeautifulSoup(d_res.text, "html.parser")
                                
                                # ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
                                pdf_btn = d_soup.find("a", href=re.compile(r"download|view|filekey|attach|\.pdf", re.IGNORECASE))
                                if not pdf_btn:
                                    pdf_btn = d_soup.find("a", string=re.compile("ë¦¬í¬íŠ¸ë³´ê¸°|PDF|ë‹¤ìš´ë¡œë“œ|ë³´ê¸°", re.IGNORECASE))
                                if not pdf_btn:
                                    pdf_btn = d_soup.find("a", class_=re.compile("pdf|download|report", re.IGNORECASE))
                                
                                if pdf_btn:
                                    pdf_href = pdf_btn.get("href", "")
                                    if pdf_href.startswith("http"):
                                        pdf_url = pdf_href
                                    elif pdf_href.startswith("/"):
                                        pdf_url = "https://finance.naver.com" + pdf_href
                                    else:
                                        pdf_url = "https://finance.naver.com/" + pdf_href
                                    print(f"      [DEBUG PDF] ìƒì„¸ í˜ì´ì§€ì—ì„œ PDF ë°œê²¬: {pdf_url[:80]}...")
                            except:
                                pass
                    except Exception as e:
                        pdf_url = None
                    
                    # v11.1: PDFê°€ ì—†ìœ¼ë©´ detail_urlì„ HTML ì†ŒìŠ¤ë¡œ ì‚¬ìš© (HTML fallback)
                    # URL ìœ íš¨ì„± ê²€ì‚¬
                    valid_url = detail_url
                    if not detail_url or not detail_url.startswith("http"):
                        valid_url = None
                    
                    # PDFê°€ ì—†ëŠ” ê²½ìš°, HTML URLë¡œ ì‚¬ìš© (ì‹ í•œíˆ¬ì ë“± HTML ë¦¬í¬íŠ¸ ëŒ€ì‘)
                    if not pdf_url:
                        # detail_urlì„ HTML URLë¡œ ì‚¬ìš©
                        if detail_url and ("read.naver" in detail_url or "/research/" in detail_url):
                            valid_url = detail_url
                        elif valid_url and "/item/" in valid_url:
                            # /item/ì€ ì¢…ëª© í˜ì´ì§€ì´ë¯€ë¡œ ì œì™¸
                            valid_url = None
                    else:
                        # PDFê°€ ìˆìœ¼ë©´ /item/ íŒ¨í„´ ì œì™¸
                        if valid_url and "/item/" in valid_url:
                            valid_url = None
                    
                    reports.append({
                        "source": "ë„¤ì´ë²„",
                        "category": cat,
                        "title": title_tag.get_text(strip=True),
                        "company": company,
                        "date": date,
                        "url": valid_url,
                        "pdf_url": pdf_url
                    })
                print(f"   [OK] {cat}: {len([r for r in reports if r['category'] == cat])}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                time.sleep(1)
        finally:
            driver.quit()
        print(f"[OK] ë„¤ì´ë²„: {len(reports)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return str(reports)

class HankyungScraperTool(BaseTool):
    name: str = "Hankyung Scraper Tool"
    description: str = "í•œê²½ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"
    
    def _run(self) -> str:
        """í•œê²½ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"""
        url = "https://consensus.hankyung.com/analysis/list"
        reports = []
        print(f"[DEBUG] í•œê²½ ìˆ˜ì§‘ ì‹œì‘ - ê²€ìƒ‰ ë‚ ì§œ: {target_dates[:3]}")
        try:
            res = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")
            rows = soup.select("table tbody tr")
            print(f"[DEBUG] í•œê²½: {len(rows)}ê°œ row ë°œê²¬")
            row_count = 0
            collected_count = 0
            for row in rows:
                cols = row.find_all("td")
                if len(cols) < 4: 
                    continue
                date_raw = cols[3].get_text(strip=True)
                # ë‚ ì§œ í˜•ì‹ í†µì¼ (YYYY-MM-DD â†’ YYYY.MM.DD, YY-MM-DD â†’ YY.MM.DD)
                date = date_raw.replace("-", ".")
                # ë‚ ì§œ í•„í„°: target_dates ëª©ë¡ì— ìˆëŠ” ë‚ ì§œë§Œ ìˆ˜ì§‘
                if date not in target_dates:
                    continue
                title_tag = cols[0].find("a")
                if not title_tag:
                    continue
                pdf_tag = row.find("a", href=re.compile(r"\.pdf$"))
                pdf_url = "https://consensus.hankyung.com" + pdf_tag["href"] if pdf_tag else None
                reports.append({
                    "source": "í•œê²½ì»¨ì„¼ì„œìŠ¤",
                    "category": cols[2].get_text(strip=True),
                    "title": title_tag.get_text(strip=True),
                    "company": cols[1].get_text(strip=True),
                    "date": date,
                    "url": "https://consensus.hankyung.com" + title_tag["href"],
                    "pdf_url": pdf_url
                })
                collected_count += 1
                time.sleep(0.5)
            print(f"   [OK] í•œê²½: {collected_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ í•œê²½ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return str(reports)

# ----------------------------------------------------------
# 2ï¸âƒ£ í‚¤ì›Œë“œ ë¶„ì„ (ë‚ ì§œ ì œì™¸)
# ----------------------------------------------------------
class PythonAnalyzerTool(BaseTool):
    name: str = "Python Analyzer Tool"
    description: str = "ë¦¬í¬íŠ¸ ì œëª© ê¸°ë°˜ í‚¤ì›Œë“œ/ì¹´í…Œê³ ë¦¬ ë¶„ì„"
    
    def _run(self, reports_str: str) -> str:
        """í‚¤ì›Œë“œ ë° ì¹´í…Œê³ ë¦¬ ë¶„ì„"""
        try:
            reports = eval(reports_str)
            df = pd.DataFrame(reports).drop_duplicates(subset=["title", "company"])
            
            # ë‹¨ì–´ ì¶”ì¶œ
            words = sum([re.findall(r"[ê°€-í£A-Za-z0-9]{2,12}", t) for t in df["title"]], [])
            
            # ë‚ ì§œ ê´€ë ¨ í‚¤ì›Œë“œ ì œì™¸ (10, 24, 26, 10ì›”, 2025 ë“±)
            today = datetime.now().day
            month = datetime.now().month
            year = datetime.now().year
            date_pattern = re.compile(r"(\d{1,2}ì›”|\d{1,2}ì¼|20\d{2}|\d{2}\.\d{2}|\d{4}\.\d{2}\.\d{2})")
            
            # ìˆ«ì ì „ìš© íŒ¨í„´ (ëª¨ë“  ìˆ«ì ì œì™¸)
            number_pattern = re.compile(r'^\d+$')
            
            stop_words = {
                "ë¦¬í¬íŠ¸", "ë¶„ì„", "ì „ë§", "íˆ¬ì", "ê²½ì œ", "ì‚°ì—…", "ì´ìŠˆ",
                str(today), str(month), str(year), f"{month}ì›”", "2025", "25", "24", "10", "26",
                "Weekly", "Preview", "Monitor", "Daily", "ì£¼ê°„", "ì£¼ì°¨", "ì¼ë³´",
                "China", "Weekly", "3Q25", "4ì£¼ì°¨", "10ì›”", "11ì›”", "12ì›”"
            }
            
            filtered_words = [
                w for w in words
                if w not in stop_words 
                and not date_pattern.search(w) 
                and not number_pattern.match(w)  # ìˆœìˆ˜ ìˆ«ì ì œì™¸
                and len(w) >= 2
                and w.isalnum()  # ì˜ë¬¸ì/í•œê¸€ë§Œ í—ˆìš©
            ]
            
            counter = Counter(filtered_words)
            
            result = {
                "total_reports": len(df),
                "top_keywords": ", ".join([f"{k}({v}íšŒ)" for k, v in counter.most_common(10)]),
                "category_summary": df["category"].value_counts().to_dict(),
                "reports": df.to_dict("records")  # ë¦¬í¬íŠ¸ ì „ì²´ ì •ë³´ í¬í•¨
            }
            return str(result)
        except Exception as e:
            return f"ë¶„ì„ ì‹¤íŒ¨: {e}"

# ----------------------------------------------------------
# 3ï¸âƒ£ ê° ë¦¬í¬íŠ¸ë³„ í•µì‹¬ 1ì¤„ ìš”ì•½ (PDF ë‚´ìš© í¬í•¨)
# ----------------------------------------------------------
class ReportSummarizerTool(BaseTool):
    name: str = "Report Summarizer Tool"
    description: str = "ì „ì²´ ë¦¬í¬íŠ¸ ì „ìˆ˜ ìš”ì•½ (ë³‘ë ¬ ì²˜ë¦¬)"
    
    def _extract_pdf_text(self, pdf_url: str) -> str:
        """PDF ë³¸ë¬¸ ì¶”ì¶œ (v11.1: PDF URL whitelist ê²€ì¦ ê°•í™”)"""
        try:
            # PDF URL ìœ íš¨ì„± ê²€ì¦
            if not pdf_url or not isinstance(pdf_url, str):
                return ""
            
            # v11.1: PDF URL whitelist ê¸°ë°˜ ê²€ì¦ (ë¨¼ì € whitelist í™•ì¸)
            valid_pdf_patterns = [
                r'stock\.pstatic\.net/stock-research/.*\.pdf',
                r'pstatic\.net/stock-research/.*\.pdf',
            ]
            is_valid = any(re.search(p, pdf_url) for p in valid_pdf_patterns)
            if not is_valid:
                print(f"      [DEBUG PDF] whitelist ë¶ˆì¼ì¹˜, PDFë¡œ ì¸ì • ë¶ˆê°€: {pdf_url[:80]}")
                return ""  # whitelistì— ì—†ìœ¼ë©´ PDFê°€ ì•„ë‹˜
            
            # Phase 1 (v11.0): ì¢…ëª©/ì°¨íŠ¸ í˜ì´ì§€ ê°•ë ¥ ì°¨ë‹¨ (URL íŒ¨í„´ìœ¼ë¡œ ì„ ì°¨ë‹¨)
            invalid_patterns = [
                r'/(item|chart|quote|news|frgn)/',       # ê¸°ì¡´ íŒ¨í„´
                r'finance\.naver\.com/item/',            # ë„¤ì´ë²„ ì¢…ëª© í˜ì´ì§€
                r'\.frgn\.naver',                        # ì™¸êµ­ì¸ í˜ì´ì§€
                r'/item/frgn',                           # ì¢…ëª© ì™¸êµ­ì¸ í˜ì´ì§€
            ]
            
            for pattern in invalid_patterns:
                if re.search(pattern, pdf_url, re.I):
                    print(f"      [DEBUG PDF] ê¸ˆì§€ëœ URL íŒ¨í„´ ê°ì§€: {pdf_url[:80]}")
                    return ""
            
            # URL íŒŒë¼ë¯¸í„° ì œê±° (query string, fragment ì œê±°)
            original_url = pdf_url
            pdf_url = pdf_url.split("?")[0].split("#")[0]
            
            # ê³µë°± ì œê±° ë¨¼ì € (ì¤‘ìš”!)
            pdf_url_stripped = pdf_url.strip()
            
            # PDF URL ë³´ì • (`.p` â†’ `.pdf` ìë™ ì¶”ê°€) - ê°œì„  ë²„ì „
            if pdf_url_stripped and not pdf_url_stripped.lower().endswith(".pdf"):
                # URLì´ ì˜ë¦° ê²½ìš° (.p ë˜ëŠ” .pdë¡œ ëë‚˜ëŠ” ê²½ìš°)
                if pdf_url_stripped.endswith(".p"):
                    pdf_url = pdf_url_stripped[:-1] + "pdf"  # .p â†’ .pdf (ë²„ê·¸ ìˆ˜ì •)
                elif pdf_url_stripped.endswith(".pd"):
                    pdf_url = pdf_url_stripped[:-2] + "pdf"  # .pd â†’ .pdf
                # pstatic URL íŒ¨í„´ íŠ¹ë³„ ì²˜ë¦¬ (ì ìœ¼ë¡œ ëë‚˜ì§€ ì•ŠëŠ” ê²½ìš°ë§Œ)
                elif "pstatic.net" in pdf_url_stripped and not pdf_url_stripped.endswith("."):
                    pdf_url = pdf_url_stripped + ".pdf"
                # ìˆ«ìë¡œ ëë‚˜ëŠ” URLì—ë„ .pdf ìë™ ì¶”ê°€
                elif re.search(r'/\d+$', pdf_url_stripped):
                    pdf_url = pdf_url_stripped + ".pdf"
                else:
                    pdf_url = pdf_url_stripped
            else:
                pdf_url = pdf_url_stripped
            
            print(f"      [DEBUG PDF] ì›ë³¸: {original_url[:80]} â†’ ìˆ˜ì •: {pdf_url[:80]}")
            
            # HTTP ìš”ì²­ ì‹œë„ (ë‹¤ì¤‘ fallback)
            res = None
            attempts = []
            
            # 1. ë³´ì •ëœ URLë¶€í„° ì‹œë„
            if pdf_url_stripped != pdf_url:
                attempts.append(pdf_url)
                print(f"      [DEBUG PDF] ë³´ì • URL ì¶”ê°€: {pdf_url[:80]}")
            
            # 2. ì›ë³¸ URL ì‹œë„
            attempts.append(pdf_url_stripped)
            
            # 3. .p/.pd íŒ¨í„´ì´ë©´ ìˆ˜ì •ë³¸ ì¶”ê°€ ì‹œë„
            if pdf_url_stripped.endswith(".p") and pdf_url_stripped not in attempts:
                fixed_p = pdf_url_stripped[:-1] + "pdf"  # ë²„ê·¸ ìˆ˜ì •: df â†’ pdf
                attempts.append(fixed_p)
                print(f"      [DEBUG PDF] .p ìˆ˜ì •ë³¸ ì¶”ê°€: {fixed_p[:80]}")
            elif pdf_url_stripped.endswith(".pd") and pdf_url_stripped not in attempts:
                fixed_pd = pdf_url_stripped[:-2] + "pdf"
                attempts.append(fixed_pd)
                print(f"      [DEBUG PDF] .pd ìˆ˜ì •ë³¸ ì¶”ê°€: {fixed_pd[:80]}")
            
            # 4. í™•ì¥ì ì—†ëŠ” ê²½ìš° .pdf ì¶”ê°€ ì‹œë„
            if not pdf_url_stripped.endswith(".pdf") and not pdf_url_stripped.endswith(".p") and not pdf_url_stripped.endswith(".pd"):
                attempts.append(pdf_url_stripped + ".pdf")
                print(f"      [DEBUG PDF] .pdf ì¶”ê°€ ì‹œë„: {(pdf_url_stripped + '.pdf')[:80]}")
            
            for i, attempt_url in enumerate(attempts):
                try:
                    print(f"      [DEBUG PDF] ì‹œë„ {i+1}/{len(attempts)}: {attempt_url[:80]}")
                    res = requests.get(attempt_url, headers=HEADERS, timeout=15, stream=True)
                    if res.status_code == 200:
                        pdf_url = attempt_url
                        if len(attempts) > 1:
                            print(f"      [DEBUG PDF] âœ“ ì„±ê³µ! {attempt_url[:80]}")
                        break
                    else:
                        print(f"      [DEBUG PDF] HTTP {res.status_code}")
                except Exception as e:
                    print(f"      [DEBUG PDF] ì˜ˆì™¸: {str(e)[:50]}")
                    continue
            
            if not res or res.status_code != 200:
                print(f"      [DEBUG PDF] HTTP {res.status_code if res else 'None'} - ëª¨ë“  ì‹œë„ ì‹¤íŒ¨")
                
                # v10.5: ë³´ì • ì‹¤íŒ¨í•œ .p, .pdëŠ” ì°¨ë‹¨
                if pdf_url.endswith(".p") or pdf_url.endswith(".pd"):
                    print(f"      [DEBUG PDF] ë³´ì • ì‹¤íŒ¨, ì˜ë¦° í™•ì¥ì ì°¨ë‹¨: {pdf_url[:80]}")
                    return ""
                
                return ""
            
            # Content-Type ê²€ì¦ ì™„í™” (PDFê°€ ì•„ë‹ˆì–´ë„ ì‹œë„)
            content_type = res.headers.get('content-type', '').lower()
            if 'pdf' not in content_type and not pdf_url.endswith('.pdf'):
                print(f"      [DEBUG PDF] Content-Type: {content_type} (PDF ì•„ë‹˜)")
                
                # HTML ì‘ë‹µì¸ ê²½ìš° ì¬ì‹œë„
                if content_type.startswith('text/html'):
                    print(f"      [DEBUG PDF] HTML ì‘ë‹µ â†’ URL ì¬êµ¬ì„± ì‹œë„")
                    # .pdf ìë™ ì¶”ê°€ ì‹œë„
                    alt_pdf = pdf_url.split("?")[0] + ".pdf"
                    try:
                        res_alt = requests.get(alt_pdf, headers=HEADERS, timeout=10)
                        if res_alt.status_code == 200 and 'pdf' in res_alt.headers.get('content-type', '').lower():
                            res = res_alt
                            pdf_url = alt_pdf
                            print(f"      [DEBUG PDF] ì¬êµ¬ì„± ì„±ê³µ: {alt_pdf[:80]}")
                        else:
                            print(f"      [DEBUG PDF] HTML ì¬ì‹œë„ ì‹¤íŒ¨ â†’ PDF ì•„ë‹˜")
                            return ""
                    except Exception as e:
                        print(f"      [DEBUG PDF] HTML ì¬ì‹œë„ ì˜ˆì™¸: {str(e)[:50]}")
                        return ""
            
            # íŒŒì¼ëª…ì„ ê³ ìœ í•˜ê²Œ ìƒì„± (ë™ì‹œ ì ‘ê·¼ ë°©ì§€)
            import uuid
            temp_file = f"temp_{uuid.uuid4().hex[:8]}.pdf"
            
            try:
                with open(temp_file, "wb") as f:
                    f.write(res.content)
                
                with fitz.open(temp_file) as pdf:
                    text = ""
                    total = len(pdf)
                    pages = list(range(min(5, total))) + list(range(max(0, total - 3), total))
                    for p in sorted(set(pages)):
                        text += pdf[p].get_text()
                
                print(f"      [DEBUG PDF] ì¶”ì¶œ ì„±ê³µ: {len(text)}ì")
                
                # íŒŒì¼ ë‹«íŒ í›„ ì‚­ì œ
                import time
                time.sleep(0.1)  # íŒŒì¼ í•¸ë“¤ í•´ì œ ëŒ€ê¸°
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                
                return re.sub(r"\s+", " ", text.strip())[:3500]
            except Exception as pdf_error:
                print(f"      [DEBUG PDF] íŒŒì‹± ì‹¤íŒ¨: {pdf_error}")
                return ""
        except Exception as e:
            print(f"      [PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}]")
            return ""
    
    def _extract_html_text(self, url: str, company: str = "") -> str:
        """PDFê°€ ì—†ì„ ê²½ìš° HTML ë³¸ë¬¸ í¬ë¡¤ë§ (Seleniumìœ¼ë¡œ JS ë Œë”ë§ëœ í˜ì´ì§€) - v10.0"""
        try:
            print(f"      [DEBUG HTML] URL: {url[:80]}")
            # Seleniumìœ¼ë¡œ JS ë Œë”ë§ëœ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
            driver = create_selenium_driver()
            driver.implicitly_wait(5)  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
            driver.get(url)
            time.sleep(3)  # JS ë¡œë”© ëŒ€ê¸°
            
            # v10.4: 404 ì—ëŸ¬ í˜ì´ì§€ ê°ì§€ ê°•í™” (ë‹¤ì¤‘ ì¸ì½”ë”©)
            page_title = driver.title
            page_size = len(driver.page_source)
            print(f"      [DEBUG HTML] í˜ì´ì§€ íƒ€ì´í‹€: {page_title}")
            print(f"      [DEBUG HTML] í˜ì´ì§€ í¬ê¸°: {page_size} ì")
            
            # ë‹¤ì¤‘ ì¸ì½”ë”© ê²€ì‚¬ (EUC-KR + UTF-8)
            page_raw = driver.page_source
            try:
                page_euckr = page_raw.encode('euc-kr', errors='ignore').decode('euc-kr', errors='ignore')
            except:
                page_euckr = page_raw
            
            # v10.9: 404 ê°ì§€ ë‹¨ìˆœí™” (ê³¼ë„í•œ í•„í„°ë§ ì œê±°)
            is_404 = any(keyword in page_raw for keyword in [
                "í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "ì°¾ìœ¼ì‹œëŠ” ëª¨ë“  ì •ë³´",
                "404 Not Found"
            ]) or ("404" in page_title and "ë„¤ì´ë²„" in page_title)
            
            # Phase 2 (v11.0): meta refresh ì¶”ì  + ì¬ì‹œë„ ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            redirect_count = 0
            max_redirects = 3
            visited_urls = set()  # ë¬´í•œ ë£¨í”„ ë°©ì§€: ë°©ë¬¸í•œ URL ì €ì¥
            
            while not is_404 and redirect_count < max_redirects:
                try:
                    current_url = driver.current_url
                    
                    # ë¬´í•œ ë£¨í”„ ê°ì§€: ê°™ì€ URLì„ ë‹¤ì‹œ ë°©ë¬¸í•˜ë©´ ì¤‘ë‹¨
                    if current_url in visited_urls:
                        print(f"      [DEBUG HTML] ë¬´í•œ ë£¨í”„ ê°ì§€ (ë™ì¼ URL ì¬ë°©ë¬¸): {current_url[:80]}")
                        break
                    visited_urls.add(current_url)
                    
                    soup_page = BeautifulSoup(driver.page_source[:10000], "html.parser")
                    meta_refresh = soup_page.find("meta", attrs={"http-equiv": re.compile("refresh", re.I)})
                    
                    if meta_refresh and "url=" in meta_refresh.get("content", "").lower():
                        content = meta_refresh.get("content", "")
                        redirect_url_match = re.search(r'url=([^";\s]+)', content, re.I)
                        
                        if redirect_url_match:
                            redirect_url = redirect_url_match.group(1).strip()
                            if not redirect_url.startswith("http"):
                                redirect_url = urljoin(current_url, redirect_url)
                            
                            # ë‹¤ìŒ URLì´ ì´ë¯¸ ë°©ë¬¸í•œ URLì´ë©´ ì¤‘ë‹¨ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
                            if redirect_url in visited_urls:
                                print(f"      [DEBUG HTML] ë¬´í•œ ë£¨í”„ ê°ì§€ (ì´ë¯¸ ë°©ë¬¸í•œ URL): {redirect_url[:80]}")
                                break
                            
                            print(f"      [DEBUG HTML] meta refresh {redirect_count+1}íšŒ: {redirect_url[:80]}")
                            driver.get(redirect_url)
                            time.sleep(2)
                            redirect_count += 1
                            continue
                    
                    # meta refresh ì—†ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ
                    break
                    
                except Exception as redirect_e:
                    print(f"      [DEBUG HTML] ë¦¬ë‹¤ì´ë ‰íŠ¸ ì˜¤ë¥˜: {str(redirect_e)[:50]}")
                    break
            
            if is_404:
                print(f"      [ERROR] 404 ì—ëŸ¬ í˜ì´ì§€ ê°ì§€: {url}")
                # 404 í˜ì´ì§€ ë””ë²„ê¹… ì €ì¥
                if "ì‹ í•œ" in company:
                    html_content = driver.page_source
                    if not os.path.exists("debug_html"):
                        os.makedirs("debug_html")
                    safe_company = re.sub(r'[^\w\s-]', '', company)[:20]
                    debug_file = f"debug_html/404_{safe_company}_{int(time.time())}.html"
                    with open(debug_file, "w", encoding="utf-8") as f:
                        f.write(html_content)
                    print(f"      [DEBUG HTML] 404 í˜ì´ì§€ ì €ì¥: {debug_file}")
                driver.quit()
                return ""
            
            # === v10.4: ë””ë²„ê·¸ HTML ì €ì¥ (ì‹ í•œíˆ¬ì ì „ìš©, ì •ìƒ í˜ì´ì§€ë§Œ) ===
            if "ì‹ í•œ" in company:
                html_content = driver.page_source
                if not os.path.exists("debug_html"):
                    os.makedirs("debug_html")
                safe_company = re.sub(r'[^\w\s-]', '', company)[:20]
                debug_file = f"debug_html/ok_{safe_company}_{int(time.time())}.html"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(html_content)
                print(f"      [DEBUG HTML] ì •ìƒ í˜ì´ì§€ ì €ì¥: {debug_file}")
                print(f"      [DEBUG HTML] í˜ì´ì§€ í¬ê¸°: {len(html_content)} ì")
            
            # ì‹ í•œíˆ¬ì íŠ¹í™” ì„ íƒì ì²˜ë¦¬ (ë‚˜ì¤‘ì— ì‚¬ìš©)
            if False:  # ì„ì‹œ ë¹„í™œì„±í™”
                print(f"      [DEBUG HTML] ì‹ í•œíˆ¬ìì¦ê¶Œ ë¦¬í¬íŠ¸ ê°ì§€ (company: {company})")
                content_selectors = [
                    # ì‹ í•œíˆ¬ì íŠ¹í™” ì„ íƒì (ìš°ì„ ìˆœìœ„ ë†’ê²Œ)
                    "div.view_cont",      # ì‹ í•œíˆ¬ì ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ
                    "td.view_cont",       # ì‹ í•œíˆ¬ì í…Œì´ë¸” ì…€
                    "div.article_content", # ê¸°ì‚¬ ë³¸ë¬¸
                    "div.content_body",   # ë³¸ë¬¸ ì˜ì—­
                    "div#content_detail", # ìƒì„¸ ë³¸ë¬¸ ID
                    "div.report_view",    # ë¦¬í¬íŠ¸ ë·°
                    "div.article_view",   # ê¸°ì‚¬ ë·°
                    # ë„¤ì´ë²„ í‘œì¤€ ì„ íƒì
                    "td.view_cnt",
                    "div.view_cnt",
                    "td.view_content",
                    "table.view",
                    "div.view_con",
                    # ì¼ë°˜ ì„ íƒì
                    "div.report-content",
                    "div.report-body",
                    "div.viewer-content",
                    "div.article-content",
                    "td.content",
                    "div.content",
                    "#articleBody",
                    "article"
                ] + content_selectors
            
            # iframeì´ ìˆëŠ” ê²½ìš° ë‚´ë¶€ ë¬¸ì„œ ì ‘ê·¼ (v10.0: ë‹¤ì¤‘ ë°©ë²• íƒìƒ‰)
            html = None
            try:
                # v10.0: ë‹¤ì¤‘ ë°©ë²•ìœ¼ë¡œ iframe íƒìƒ‰
                iframes = driver.find_elements(By.TAG_NAME, "iframe")
                
                # ë°©ë²• 2: XPathë¡œ iframe ì°¾ê¸°
                if not iframes:
                    iframes = driver.find_elements(By.XPATH, "//iframe")
                
                # ë°©ë²• 3: CSS ì„ íƒìë¡œ iframe ì°¾ê¸°
                if not iframes:
                    iframes = driver.find_elements(By.CSS_SELECTOR, "iframe[id*='view'], iframe[src]")
                
                # ë°©ë²• 4: frame íƒœê·¸ë„ ì°¾ê¸°
                if not iframes:
                    iframes = driver.find_elements(By.TAG_NAME, "frame")
                
                print(f"      [DEBUG HTML] iframe íƒìƒ‰ ì™„ë£Œ: {len(iframes)}ê°œ ë°œê²¬")
                
                if iframes:
                    print(f"      [DEBUG HTML] iframe {len(iframes)}ê°œ ë°œê²¬")
                    
                    # iframe ìˆœíšŒí•˜ë©° ë³¸ë¬¸ ì°¾ê¸°
                    for idx, frame in enumerate(iframes):
                        try:
                            # v10.9: ì‹ í•œíˆ¬ì iframe ì§ì ‘ ì ‘ê·¼ + Mobile UA ì ìš©
                            src = frame.get_attribute("src")
                            if src and "shinhaninvest" in src.lower():
                                print(f"      [DEBUG HTML] ì‹ í•œ iframe src ê°ì§€: {src[:80]}")
                                
                                # Mobile User-Agentë¡œ ì „í™˜
                                mobile_ua = ("Mozilla/5.0 (Linux; Android 10; SM-G973F) "
                                           "AppleWebKit/537.36 (KHTML, like Gecko) "
                                           "Chrome/124.0.0.0 Mobile Safari/537.36")
                                driver.execute_cdp_cmd("Network.setUserAgentOverride", 
                                                      {"userAgent": mobile_ua})
                                print(f"      [DEBUG HTML] Mobile UA ì ìš©")
                                
                                driver.get(src)  # iframe srcë¡œ ì§ì ‘ ì´ë™
                                time.sleep(2)
                                html = driver.page_source
                                if len(html) > 1000:
                                    break
                                
                                # ì›ë˜ í˜ì´ì§€ë¡œ ë³µê·€
                                driver.execute_cdp_cmd("Network.setUserAgentOverride", 
                                                      {"userAgent": HEADERS['User-Agent']})
                                driver.back()
                                time.sleep(1)
                                continue
                            
                            # ê¸°ì¡´ í”„ë ˆì„ ì „í™˜ ë°©ì‹ (fallback)
                            driver.switch_to.frame(frame)
                            time.sleep(2)  # iframe ë Œë”ë§ ëŒ€ê¸° (1ì´ˆ â†’ 2ì´ˆ)
                            
                            # ë‚´ë¶€ iframe í™•ì¸ (ì´ì¤‘ êµ¬ì¡°)
                            inner_iframes = driver.find_elements("tag name", "iframe")
                            if inner_iframes:
                                print(f"      [DEBUG HTML] ë‚´ë¶€ iframe {len(inner_iframes)}ê°œ ë°œê²¬")
                                for inner_idx, inner_frame in enumerate(inner_iframes):
                                    try:
                                        driver.switch_to.frame(inner_frame)
                                        time.sleep(2)  # ë‚´ë¶€ iframe ë Œë”ë§ ëŒ€ê¸°
                                        
                                        # iframe ë‚´ë¶€ HTML ê°€ì ¸ì˜¤ê¸°
                                        candidate_html = driver.page_source
                                        
                                        # ë³¸ë¬¸ ìœ íš¨ì„± ê²€ì‚¬
                                        if len(candidate_html) > 1000:
                                            # ë³¸ë¬¸ í‚¤ì›Œë“œ í™•ì¸
                                            if any(keyword in candidate_html for keyword in ["ê²½ìŸì‚¬", "ì´ìµë¥ ", "ë§¤ì¶œ", "ì „ë§", "ì¦ê¶Œ", "ë¦¬í¬íŠ¸"]):
                                                print(f"      [DEBUG HTML] ë‚´ë¶€ iframe #{inner_idx} ë³¸ë¬¸ í™•ì¸: {len(candidate_html)}ì")
                                                html = candidate_html
                                                break
                                        
                                        driver.switch_to.parent_frame()
                                    except Exception as inner_e:
                                        print(f"      [DEBUG HTML] ë‚´ë¶€ iframe #{inner_idx} ì „í™˜ ì‹¤íŒ¨: {str(inner_e)[:50]}")
                                        try:
                                            driver.switch_to.parent_frame()
                                        except:
                                            pass
                                        continue
                            
                            # ë‚´ë¶€ iframeì—ì„œ ë³¸ë¬¸ ëª» ì°¾ì•˜ìœ¼ë©´ ì™¸ë¶€ iframeì—ì„œ ì‹œë„
                            if not html or len(html) < 1000:
                                candidate_html = driver.page_source
                                if len(candidate_html) > 1000:
                                    if any(keyword in candidate_html for keyword in ["ê²½ìŸì‚¬", "ì´ìµë¥ ", "ë§¤ì¶œ", "ì „ë§", "ì¦ê¶Œ", "ë¦¬í¬íŠ¸"]):
                                        print(f"      [DEBUG HTML] ì™¸ë¶€ iframe #{idx} ë³¸ë¬¸ í™•ì¸: {len(candidate_html)}ì")
                                        html = candidate_html
                            
                            # ì„±ê³µí•˜ë©´ íƒˆì¶œ
                            if html and len(html) > 1000:
                                driver.switch_to.default_content()
                                break
                            
                            # ê¸°ë³¸ í”„ë ˆì„ìœ¼ë¡œ ë³µê·€
                            driver.switch_to.default_content()
                            
                        except Exception as frame_e:
                            print(f"      [DEBUG HTML] iframe #{idx} ì „í™˜ ì‹¤íŒ¨: {str(frame_e)[:50]}")
                            try:
                                driver.switch_to.default_content()
                            except:
                                pass
                            continue
                    
                    # iframe ì „í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í˜ì´ì§€ ì‚¬ìš©
                    if not html or len(html) < 1000:
                        driver.switch_to.default_content()
                        html = driver.page_source
                        print(f"      [DEBUG HTML] iframe ì‹¤íŒ¨, ê¸°ë³¸ í˜ì´ì§€ ì‚¬ìš©: {len(html)}ì")
                else:
                    html = driver.page_source
                    print(f"      [DEBUG HTML] iframe ì—†ìŒ, ê¸°ë³¸ í˜ì´ì§€ ì‚¬ìš©: {len(html)}ì")
                    
            except Exception as iframe_error:
                print(f"      [DEBUG HTML] iframe ì²˜ë¦¬ ì˜¤ë¥˜: {str(iframe_error)[:50]}")
                try:
                    driver.switch_to.default_content()
                except:
                    pass
                html = driver.page_source
            
            driver.quit()
            
            soup = BeautifulSoup(html, "html.parser")
            
            # ë„¤ì´ë²„ ë¦¬í¬íŠ¸ í˜ì´ì§€ êµ¬ì¡°ì— ë§ì¶° ë³¸ë¬¸ ì¶”ì¶œ
            # ì£¼ìš” ì„¹ì…˜ ì„ íƒìë“¤ (í™•ì¥ ë²„ì „)
            content_selectors = [
                "td.view_cnt",         # ë„¤ì´ë²„ ë¦¬í¬íŠ¸ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ (í•µì‹¬ ì„ íƒì)
                "div.view_cnt",        # div í˜•íƒœì˜ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ
                "td.view_content",     # í…Œì´ë¸” ì…€ ë³¸ë¬¸ (ê²½ì œ/ì‚°ì—… ë¶„ì„ ìš°ì„ )
                "table.view",          # í…Œì´ë¸” ë·°
                "div.view_con",        # ë„¤ì´ë²„ ë¦¬í¬íŠ¸ ë³¸ë¬¸
                "div.tb_view",         # í…Œì´ë¸” í˜•ì‹ ì¶”ê°€
                "div.article_view", 
                "div.article_view_con",
                "section.article",     # ì„¹ì…˜ ê¸°ë°˜ ë³¸ë¬¸
                "div#articleBody",     # ë³¸ë¬¸ ì˜ì—­ ID
                "div#wrap_view",       # ë·° ë˜í¼
                "div#wrapContent",     # ì»¨í…ì¸  ë˜í¼
                "div#contentArea",     # ì»¨í…ì¸  ì˜ì—­
                "div.article_body",    # ê¸°ì‚¬ ë³¸ë¬¸
                "div.end_body",        # ë³¸ë¬¸ ë ë¶€ë¶„
                "div.tb_type1",        # í…Œì´ë¸” í˜•ì‹
                "div.tb_cont",         # í…Œì´ë¸” ì»¨í…ì¸ 
                "div.board_view",      # ê²Œì‹œíŒ í˜•ì‹
                "article",
                "div.content",
                "#content"
            ]
            
            # ì‹ í•œíˆ¬ìì¦ê¶Œ ì „ìš© ì„ íƒì ì¶”ê°€ (company íŒŒë¼ë¯¸í„° ì‚¬ìš©)
            if "ì‹ í•œ" in company:
                print(f"      [DEBUG HTML] ì‹ í•œíˆ¬ìì¦ê¶Œ ë¦¬í¬íŠ¸ ê°ì§€ (company: {company})")
                content_selectors = [
                    # ì‹ í•œíˆ¬ì íŠ¹í™” ì„ íƒì (ìš°ì„ ìˆœìœ„ ë†’ê²Œ)
                    "div.view_cont",      # NEW: ì‹ í•œíˆ¬ì ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ
                    "td.view_cont",       # NEW: ì‹ í•œíˆ¬ì í…Œì´ë¸” ì…€
                    "div.article_content", # NEW: ê¸°ì‚¬ ë³¸ë¬¸
                    "div.content_body",   # NEW: ë³¸ë¬¸ ì˜ì—­
                    "div#content_detail", # NEW: ìƒì„¸ ë³¸ë¬¸ ID
                    "div.report_view",    # NEW: ë¦¬í¬íŠ¸ ë·°
                    "div.article_view",   # NEW: ê¸°ì‚¬ ë·°
                    # ë„¤ì´ë²„ í‘œì¤€ ì„ íƒì
                    "td.view_cnt",
                    "div.view_cnt",
                    "td.view_content",
                    "table.view",
                    "div.view_con",
                    # ì¼ë°˜ ì„ íƒì
                    "div.report-content",
                    "div.report-body",
                    "div.viewer-content",
                    "div.article-content",
                    "td.content",
                    "div.content",
                    "#articleBody",
                    "article"
                ] + content_selectors
            
            # v10.5: ë¹ ë¥¸ ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ (ìš°ì„  ì‹œë„)
            text = ""
            print(f"      [DEBUG HTML] ì„ íƒì {len(content_selectors)}ê°œ ì¤‘ ë§¤ì¹­ ì‹œë„...")
            for idx, selector in enumerate(content_selectors[:5]):  # ì²˜ìŒ 5ê°œë§Œ ë¹ ë¥´ê²Œ ì‹œë„
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(separator="\n").strip()
                    if len(text) > 100:
                        print(f"      [DEBUG HTML] OK ì„ íƒì #{idx+1} '{selector}' ë§¤ì¹­ ì„±ê³µ: {len(text)}ì")
                        break
                    else:
                        text = ""  # ê³„ì† ì‹œë„
                else:
                    if idx < 3:
                        print(f"      [DEBUG HTML] FAIL ì„ íƒì #{idx+1} '{selector}' ë§¤ì¹­ ì‹¤íŒ¨")
            
            # ì„ íƒì ì‹¤íŒ¨ ì‹œ í´ë˜ìŠ¤ ê¸°ë°˜ ê²€ìƒ‰ (v10.5 ì‹ ê·œ)
            if not text or len(text) < 100:
                print(f"      [DEBUG HTML] ì„ íƒì ì‹¤íŒ¨, í´ë˜ìŠ¤ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ fallback")
                text_blocks = soup.find_all(["td", "div"], class_=re.compile(r"view|content|article|report", re.I))
                texts = [t.get_text(strip=True) for t in text_blocks if len(t.get_text(strip=True)) > 100]
                if texts:
                    text = max(texts, key=len)
                    print(f"      [DEBUG HTML] í´ë˜ìŠ¤ ê¸°ë°˜ ê²€ìƒ‰ ì„±ê³µ: {len(text)}ì")
            
            # ìœ„ ì„ íƒìë¡œ ëª» ì°¾ìœ¼ë©´ ì „ì²´ ë³¸ë¬¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
            if not text or len(text) < 100:  # 200ì â†’ 100ìë¡œ ì™„í™”
                if text:
                    print(f"      [DEBUG HTML] ì„ íƒìë¡œ ì¶”ì¶œí–ˆì§€ë§Œ {len(text)}ìë°–ì— ì•ˆ ë¨, fallback ì‹œë„")
                else:
                    print(f"      [DEBUG HTML] ì„ íƒì ë§¤ì¹­ ì™„ì „ ì‹¤íŒ¨, fallback ì‹œë„")
                # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
                for tag in soup(["script", "style", "nav", "footer", "header"]):
                    tag.decompose()
                text = soup.get_text(separator="\n").strip()
                print(f"      [DEBUG HTML] fallback step1: ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ {len(text)}ì")
                
                # ì—¬ì „íˆ ì§§ìœ¼ë©´ ëª¨ë“  íƒœê·¸ì—ì„œ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì°¾ê¸° (ê°œì„ )
                if not text or len(text) < 100:
                    print(f"      [DEBUG HTML] fallback step2: ì „ì²´ íƒœê·¸ ì¤‘ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ê²€ìƒ‰...")
                    longest_text = ""
                    longest_len = 0
                    
                    for tag in soup.find_all(["p", "td", "div", "article", "section", "span"]):
                        tag_text = tag.get_text(separator=" ").strip()
                        # ê´‘ê³ /ë„¤ë¹„ê²Œì´ì…˜ íŒ¨í„´ í•„í„°ë§
                        if (len(tag_text) > longest_len and 
                            len(tag_text) >= 100 and 
                            not re.search(r"ëª©ë¡|ì¡°íšŒ|ì‹ í•œíˆ¬ìì¦ê¶Œ ë¦¬ì„œì¹˜ íƒìƒ‰ê¸°|ë„¤ì´ë²„|ì‚­ì œ|ì˜¤ë¥˜|ì£¼ì‹ê±°ë˜", tag_text, re.IGNORECASE)):
                            longest_text = tag_text
                            longest_len = len(tag_text)
                    
                    if longest_text and longest_len >= 100:
                        text = longest_text
                        print(f"      [DEBUG HTML] fallback step2: ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë°œê²¬ - {len(text)}ì")
                    elif longest_text and longest_len >= 50 and "ì‹ í•œ" in company:
                        # ì‹ í•œíˆ¬ìëŠ” 50ì ì´ìƒë„ í—ˆìš©
                        text = longest_text
                        print(f"      [DEBUG HTML] fallback step2: ì‹ í•œíˆ¬ì ë³¸ë¬¸ ì¶”ì¶œ - {len(text)}ì")
                    else:
                        print(f"      [DEBUG HTML] fallback ì‹¤íŒ¨: ìµœëŒ€ {longest_len}ìë§Œ ë°œê²¬ë¨")
            
            # ê´‘ê³ /ë„¤ë¹„ê²Œì´ì…˜ í…ìŠ¤íŠ¸ í•„í„°ë§
            text = re.sub(r"\s+", " ", text.strip())
            
            # 404 ì—ëŸ¬ í˜ì´ì§€ ì²´í¬
            error_patterns = [
                r"ë°©ë¬¸í•˜ì‹œë ¤ëŠ” í˜ì´ì§€ì˜ ì£¼ì†Œê°€ ì˜ëª»",
                r"í˜ì´ì§€ì˜ ì£¼ì†Œê°€ ë³€ê²½",
                r"ì‚­ì œë˜ì—ˆê±°ë‚˜",
                r"ë„¤ì´ë²„ :: ì„¸ìƒì˜ ëª¨ë“  ì§€ì‹",
            ]
            
            for pattern in error_patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    return ""  # ì—ëŸ¬ í˜ì´ì§€ëŠ” ë¹ˆ í…ìŠ¤íŠ¸ ë°˜í™˜
            
            # ìœ íš¨í•œ ë³¸ë¬¸ì¸ì§€ íŒë‹¨ (ì‹ í•œíˆ¬ìëŠ” 50ì, ì¼ë°˜ì€ 100ì ì´ìƒ)
            min_length = 50 if "ì‹ í•œ" in company else 100
            if len(text) < min_length:
                print(f"      [DEBUG HTML] ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {len(text)}ì (ìµœì†Œ: {min_length}ì)")
                return ""
            
            # ê´‘ê³  íŒ¨í„´ ì²´í¬ (ë” ì •êµí•˜ê²Œ)
            ad_patterns = [
                r"ë„¤ì´ë²„ ì£¼ì‹ê±°ë˜ì—°ê²°.*ë¹ ë¥¸ ì£¼ë¬¸.*ë„ì™€ë“œë¦½ë‹ˆë‹¤",  # ì—°ê²°ëœ ê´‘ê³  í…ìŠ¤íŠ¸
                r"^.{0,100}ì£¼ì„.*ê²°ë¡ .*ì°¸ê³ .*$",  # ë„ˆë¬´ ì§§ì€ ë°˜ë³µ íŒ¨í„´
            ]
            
            for pattern in ad_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
                if matches and len(text) < 500:
                    # ê´‘ê³  í…ìŠ¤íŠ¸ê°€ ì£¼ìš” ë‚´ìš©ì´ê³  ì „ì²´ê°€ ì§§ìœ¼ë©´ ì œì™¸
                    return ""
            
            # ìµœì¢… ì •ì œ ë° ê¸¸ì´ ì œí•œ (3500ìë¡œ í™•ì¥)
            text = text[:3500]
            print(f"      [DEBUG HTML] ìµœì¢… ì¶”ì¶œ ì„±ê³µ: {len(text)}ì")
            return text
        except Exception as e:
            print(f"      [HTML ì¶”ì¶œ ì‹¤íŒ¨: {e}]")
            import traceback
            traceback.print_exc()
            return ""
    
    def _summarize_report(self, report: dict, idx: int, total: int) -> dict:
        """ë‹¨ì¼ ë¦¬í¬íŠ¸ ìš”ì•½ (gpt-4o-mini ì‚¬ìš©)"""
        title = report["title"]
        company = report["company"]
        category = report["category"]
        pdf_url = report.get("pdf_url")
        url = report.get("url")
        
        # v10.5: ì§„ë‹¨ ë¡œê·¸ ì¶”ê°€
        print(f"[TRACE] {idx+1}/{total} | {company} | {title[:40]}... | URLs: PDF={'O' if pdf_url else 'X'}, HTML={'O' if url else 'X'}")
        
        # PDF ë˜ëŠ” HTML í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = ""
        source_type = "ì—†ìŒ"
        
        if pdf_url:
            text = self._extract_pdf_text(pdf_url)
            if text:
                source_type = "PDF"
        
        if not text and url:
            text = self._extract_html_text(url, company=company)
            if text:
                source_type = "HTML"
        
        # GPT ìš”ì•½ (gpt-4o-mini)
        text_preview = text[:2000] if text else '[ë³¸ë¬¸ ì—†ìŒ]'
        
        # ë””ë²„ê·¸ ë¡œê·¸ (ìƒì„¸) - ASCIIë¡œë§Œ ì¶œë ¥
        if text:
            text_safe = text[:60].encode('ascii', 'ignore').decode('ascii')
            print(f"      [{source_type}] {len(text)}ì: {text_safe}...")
        else:
            title_safe = title[:40].encode('ascii', 'ignore').decode('ascii')
            print(f"\n[DIAG] {title_safe}")
            if pdf_url:
                print(f"   -> PDF URL: {pdf_url[:80]}")
            if url:
                print(f"   -> HTML URL: {url[:80]}")
            print(f"   -> ì›ì¸: PDFì™€ HTML ë‘˜ ë‹¤ ì‹œë„í–ˆìœ¼ë‚˜ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
        
        # ë³¸ë¬¸ ì—†ìœ¼ë©´ ì œëª©ê³¼ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ìœ¼ë¡œë§Œ ìš”ì•½
        if not text:
            prompt = f"""ì•„ë˜ ë¦¬í¬íŠ¸ ì œëª©ê³¼ ì¹´í…Œê³ ë¦¬ë§Œ ë³´ê³  í•µì‹¬ì„ 1ë¬¸ì¥ìœ¼ë¡œ ì¶”ì •í•˜ë¼.
ì œëª©: {title}
ì¦ê¶Œì‚¬: {company}
ì¹´í…Œê³ ë¦¬: {category}
ìš”ì•½ (ìœ ì¶”ëœ ì£¼ìš” ë‚´ìš© 1ë¬¸ì¥):"""
        else:
            prompt = f"""ì•„ë˜ ë¦¬í¬íŠ¸ë¥¼ ì½ê³  í•µì‹¬ë§Œ 1ë¬¸ì¥ìœ¼ë¡œ ì••ì¶•í•˜ë¼.
ì œëª©: {title}
ì¦ê¶Œì‚¬: {company}
ë³¸ë¬¸: {text_preview}
ìš”ì•½ (ê¸°ì—…ëª…+íˆ¬ìí¬ì¸íŠ¸ í¬í•¨):"""
        try:
            resp = client.chat.completions.create(
                model=LLM_SUMMARY,
                messages=[
                    {"role": "system", "content": "í•µì‹¬ë§Œ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½"},
                    {"role": "user", "content": prompt}
                ]
            )
            summary = resp.choices[0].message.content.strip()
        except Exception as e:
            summary = f"[ìš”ì•½ ì‹¤íŒ¨: {e}]"
        
        title_safe = title[:35].encode('ascii', 'ignore').decode('ascii')
        company_safe = company.encode('ascii', 'ignore').decode('ascii')
        print(f"[OK] [{idx+1}/{total}] {title_safe}... ({company_safe})")
        return {"title": title, "company": company, "category": category, "summary": summary}
    
    def _run(self, reports_str: str) -> str:
        """ì „ì²´ ë¦¬í¬íŠ¸ ì „ìˆ˜ ìš”ì•½ (ë³‘ë ¬ ì²˜ë¦¬)"""
        reports = eval(reports_str)
        total_reports = len(reports)
        print(f"\n[INFO] ì´ {total_reports}ê°œ ë¦¬í¬íŠ¸ ì „ìˆ˜ ìš”ì•½ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)")
        
        summaries = []
        # v10.5: ë³‘ë ¬ ì‹¤í–‰ (HTML/iframe ì ‘ê·¼ì€ ë¶€í•˜ í¼ â†’ ì›Œì»¤ ìˆ˜ ì¶•ì†Œ)
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(self._summarize_report, r, i, total_reports): i
                for i, r in enumerate(reports)
            }
            for future in as_completed(futures):
                summaries.append(future.result())
                time.sleep(0.1)
        
        print(f"\n[OK] ì´ {len(summaries)}ê°œ ë¦¬í¬íŠ¸ ìš”ì•½ ì™„ë£Œ")
        return str(summaries)

# ----------------------------------------------------------
# 4ï¸âƒ£ ìµœì¢… ë¸Œë¦¬í•‘
# ----------------------------------------------------------
class FinalBriefingTool(BaseTool):
    name: str = "Final Briefing Tool"
    description: str = "ê° ë¦¬í¬íŠ¸ ìš”ì•½ì„ ì¢…í•©í•´ íˆ¬ì ë¸Œë¦¬í•‘ ì‘ì„±"
    
    def _run(self, summaries_str: str, analysis_str: str) -> str:
        """ìµœì¢… ë¸Œë¦¬í•‘ ìƒì„±"""
        summaries = eval(summaries_str)
        analysis = eval(analysis_str)
        
        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¦¬í¬íŠ¸ ê·¸ë£¹í™”
        by_category = {}
        for s in summaries:
            cat = s.get('category', 'ê¸°íƒ€')
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(s)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½ ì •ë¦¬
        category_summaries = []
        for cat in ['íˆ¬ìì •ë³´', 'ì¢…ëª©ë¶„ì„', 'ì‚°ì—…ë¶„ì„', 'ê²½ì œë¶„ì„']:
            if cat in by_category:
                reports = by_category[cat]
                summary_texts = [f"- {r['summary']} ({r['company']})" for r in reports]
                category_summaries.append(f"\n### {cat} ({len(reports)}ê±´)\n" + "\n".join(summary_texts))
        
        prompt = f"""ìœ„ ë¦¬í¬íŠ¸ ë‚´ìš©ì„ **ì •í™•í•˜ê²Œ ì •ë¦¬**í•˜ë¼. ëª¨ë“  ì •ë³´ëŠ” ë¦¬í¬íŠ¸ ì›ë¬¸ì—ì„œ ì¶”ì¶œí•œ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ë¼.

**ê¸ˆì§€ ì‚¬í•­:**
- LLMì˜ ì¶”ë¡ , í•´ì„, ê²°ë¡  ë„ì¶œ
- "ë§¤ìˆ˜ ê¶Œê³ " ë“±ì˜ ì•¡ì…˜ ì œì•ˆ
- ëª©í‘œê°€/í˜„ì¬ê°€ ë¹„êµë¡œ ìˆ˜ìµë¥  ê³„ì‚°
- "30% ë¹„ì¤‘" ë“±ì˜ ë¹„ì¤‘ ì œì•ˆ
- "ë§¤ìˆ˜ê°€/ìµì ˆê°€/ì†ì ˆê°€" ë“± LLM ì¬ì°½ì¡°

**í•„ìˆ˜: ë¦¬í¬íŠ¸ ì›ë¬¸ ì •ë³´ë§Œ ê¸°ë¡**

{''.join(category_summaries)}

[í‚¤ì›Œë“œ ë¶„ì„]
{analysis.get("top_keywords", "N/A")}

---
**ì‘ë‹µ í˜•ì‹:**

## 1. ì¢…ëª© ë¶„ì„ (ì¢…ëª©ë³„ ì •ë¦¬)

### [ì¢…ëª©ëª…] (ì¦ê¶Œì‚¬: OOì¦ê¶Œ)
- **íˆ¬ìì˜ê²¬**: [BUY/SELL/HOLD ë“± ë¦¬í¬íŠ¸ ì›ë¬¸]
- **ëª©í‘œê°€**: [ìˆ˜ì¹˜] ([ìƒí–¥/í•˜í–¥/ìœ ì§€] - ë¦¬í¬íŠ¸ì— ëª…ì‹œëœ ê²½ìš°ë§Œ)
- **í•µì‹¬ ì „ë§**:
  1. [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš© ê·¸ëŒ€ë¡œ]
  2. [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš© ê·¸ëŒ€ë¡œ]
  3. [ì¶”ê°€ ë‚´ìš©]
- **ë¦¬ìŠ¤í¬ ìš”ì¸**: [ë¦¬í¬íŠ¸ì— ëª…ì‹œëœ ë¦¬ìŠ¤í¬ë§Œ ê·¸ëŒ€ë¡œ]

[ëª¨ë“  ì¢…ëª© ë™ì¼ í˜•ì‹ìœ¼ë¡œ ë‚˜ì—´]

---

## 2. ì‚°ì—… ë¶„ì„ (ì‚°ì—…ë³„ ì •ë¦¬)

### [ì‚°ì—…ëª…] (ì¦ê¶Œì‚¬: OOì¦ê¶Œ, OOì¦ê¶Œ ì™¸ ë‹¤ìˆ˜)
- **ì—…í™© ì „ë§**: [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš©]
- **í•µì‹¬ ì´ìŠˆ**:
  1. [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš©]
  2. [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš©]
- **ì¦ê¶Œì‚¬ ì˜ê²¬**:
  - OOì¦ê¶Œ: "[ì¦ê¶Œì‚¬ ì›ë¬¸ ê·¸ëŒ€ë¡œ]"
  - OOì¦ê¶Œ: "[ì¦ê¶Œì‚¬ ì›ë¬¸ ê·¸ëŒ€ë¡œ]"

[ëª¨ë“  ì‚°ì—… ë™ì¼ í˜•ì‹]

---

## 3. ê±°ì‹œÂ·ì‹œì¥ ì „ë§ (ì „ë¬¸ê°€ ì˜ê²¬ ì •ë¦¬)

### ì½”ìŠ¤í”¼ ì „ë§
- **OOì¦ê¶Œ**: [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš© ê·¸ëŒ€ë¡œ]
- **OOì¦ê¶Œ**: [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš© ê·¸ëŒ€ë¡œ]

### í™˜ìœ¨Â·ê¸ˆë¦¬ ì „ë§
- **OOì¦ê¶Œ**: [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš© ê·¸ëŒ€ë¡œ]
- **OOì¦ê¶Œ**: [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš© ê·¸ëŒ€ë¡œ]

### ê¸°íƒ€ ê±°ì‹œ ì´ìŠˆ
- [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš© ê·¸ëŒ€ë¡œ ë‚˜ì—´]

---

## 4. ì¤‘ìš” ì¼ì •Â·ì²´í¬í¬ì¸íŠ¸ (ë¦¬í¬íŠ¸ ìº˜ë¦°ë”)

### ì‹¤ì  ë°œí‘œ ì˜ˆì •
- [ì¢…ëª©ëª…]: [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš©]

### ì´ë²¤íŠ¸ ì¼ì •
- [ì´ë²¤íŠ¸ëª…]: [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš©]

### ì£¼ìš” ì§€í‘œ ë°œí‘œì¼
- [ì§€í‘œëª…]: [ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš©]

---

**ì‘ì„± ê·œì¹™:**
- ëª¨ë“  ì •ë³´ëŠ” ë¦¬í¬íŠ¸ ì›ë¬¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ
- "ì¦ê¶Œì‚¬: OOì¦ê¶Œ" í‘œê¸° í•„ìˆ˜
- ëª©í‘œê°€, íˆ¬ìì˜ê²¬ ë“± êµ¬ì²´ì  ìˆ˜ì¹˜ ê·¸ëŒ€ë¡œ ê¸°ë¡
- LLM ì¶”ë¡ /í•´ì„ ê¸ˆì§€, ì‚¬ì‹¤ ë‚˜ì—´ë§Œ
- "ì›í•˜ì‹œë©´", "ì¶”ê°€ë¡œ ì œê³µ" ê°™ì€ ì§ˆë¬¸ ê¸ˆì§€
- ë§ˆë¬´ë¦¬ ë¬¸êµ¬ ê¸ˆì§€"""
        
        try:
            resp = client.chat.completions.create(
                model=LLM_BRIEFING,
                messages=[
                    {"role": "system", "content": "ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ì •ë³´ ì •ë¦¬ ì „ë¬¸ê°€. ë¦¬í¬íŠ¸ ì›ë¬¸ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ì •ë¦¬ë§Œ í•œë‹¤. ì¶”ë¡ , í•´ì„, ê²°ë¡  ë„ì¶œ ê¸ˆì§€. ëª¨ë“  ì •ë³´ëŠ” ë¦¬í¬íŠ¸ ì›ë¬¸ì—ì„œ ì§ì ‘ ì¶”ì¶œí•œ ì‚¬ì‹¤ë§Œ ë‚˜ì—´. ë¶ˆí•„ìš”í•œ ì§ˆë¬¸ì´ë‚˜ ë§ˆë¬´ë¦¬ ë¬¸êµ¬ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ì•ŠìŒ."},
                    {"role": "user", "content": prompt}
                ]
            )
            body = resp.choices[0].message.content.strip()
        except Exception as e:
            body = f"[ë¸Œë¦¬í•‘ ìƒì„± ì‹¤íŒ¨: {e}]"
        
        header = f"# {today_file} ì¼ì¼ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë¸Œë¦¬í•‘\n\n*ì´ {analysis['total_reports']}ê±´ ê¸°ë°˜ / {today_display} ë°œí–‰*\n\n"
        return header + body

# ----------------------------------------------------------
# 5ï¸âƒ£ Notion ì—…ë¡œë“œ
# ----------------------------------------------------------
class NotionUploadTool(BaseTool):
    name: str = "Notion Upload Tool"
    description: str = "ìµœì¢… ë¸Œë¦¬í•‘ê³¼ ë¶„ì„ê²°ê³¼ë¥¼ Notion DBì— ì—…ë¡œë“œ"
    
    def _run(self, briefing_text: str, analysis_str: str) -> str:
        """Notionì— ì—…ë¡œë“œ"""
        try:
            analysis = eval(analysis_str)
            total_reports = analysis.get("total_reports", 0)
            
            page_data = {
                "parent": {"database_id": NOTION_DATABASE_ID},
                "properties": {
                    "Name": {"title": [{"text": {"content": f"{today_file} ì¼ì¼ ë¸Œë¦¬í•‘"}}]},
                    "Date": {"date": {"start": today_file}},
                    "ì´ ë¦¬í¬íŠ¸ ìˆ˜": {"number": total_reports},
                    "Top Keywords": {"rich_text": [{"text": {"content": analysis.get("top_keywords", "")[:2000]}}]},
                    "Category Summary": {"rich_text": [{"text": {"content": str(analysis.get("category_summary", {}))[:2000]}}]},
                },
                "children": []
            }
            
            # ë¸Œë¦¬í•‘ ë³¸ë¬¸ì„ childrenìœ¼ë¡œ ì¶”ê°€
            for i in range(0, len(briefing_text), 1800):
                page_data["children"].append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": briefing_text[i:i+1800]}}]
                    }
                })
            
            res = requests.post("https://api.notion.com/v1/pages", headers=NOTION_HEADERS, json=page_data)
            if not res.ok:
                return f"âš ï¸ Notion ì—…ë¡œë“œ ì‹¤íŒ¨: {res.status_code} - {res.text}"
            res.raise_for_status()
            parent_id = res.json().get("id", "")
            return f"[OK] Notion ì—…ë¡œë“œ ì™„ë£Œ (Page ID: {parent_id})"
        except Exception as e:
            return f"âš ï¸ Notion ì—…ë¡œë“œ ì‹¤íŒ¨: {e}"

# ----------------------------------------------------------
# 6ï¸âƒ£ ì‹¤í–‰ (Phase 3: PDF ìºì‹± ì¶”ê°€)
# ----------------------------------------------------------
def load_pdf_cache():
    """Phase 3: PDF ìºì‹œ ë¡œë“œ"""
    cache_file = "pdf_cache.json"
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_pdf_cache(cache):
    """Phase 3: PDF ìºì‹œ ì €ì¥"""
    cache_file = "pdf_cache.json"
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[WARN] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def run_daily_briefing():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Phase 3: PDF ìºì‹± ì ìš©)"""
    import sys
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    
    print(f"[START] {today_display} Daily Briefing ì‹œì‘ (v11.2 - ì‚¬ì‹¤ ê¸°ë°˜ ì •ë¦¬)")
    
    # Phase 3: PDF ìºì‹œ ë¡œë“œ
    pdf_cache = load_pdf_cache()
    print(f"[INFO] PDF ìºì‹œ ë¡œë“œ: {len(pdf_cache)}ê±´ ì €ì¥ë¨")
    
    # 1. ë¦¬í¬íŠ¸ ìˆ˜ì§‘
    print("\n[1/5] ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì¤‘...")
    naver_tool = NaverResearchScraperTool()
    hankyung_tool = HankyungScraperTool()
    naver_reports = eval(naver_tool._run())
    hankyung_reports = eval(hankyung_tool._run())
    all_reports = naver_reports + hankyung_reports
    
    if len(all_reports) == 0:
        print("[INFO] ë¦¬í¬íŠ¸ ì—†ìŒ")
        return "[INFO] ì—†ìŒ"
    
    print(f"\n[OK] ì´ {len(all_reports)}ê°œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ\n")
    
    # 2. ë¶„ì„
    print("[2/5] í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
    analyzer = PythonAnalyzerTool()
    analysis = eval(analyzer._run(str(all_reports)))
    print(f"   [OK] í‚¤ì›Œë“œ: {analysis['top_keywords'][:100]}...")
    
    # 3. ë¦¬í¬íŠ¸ë³„ ìš”ì•½
    print("\n[3/5] ë¦¬í¬íŠ¸ ìš”ì•½ ì¤‘...")
    summarizer = ReportSummarizerTool()
    summaries = eval(summarizer._run(str(analysis["reports"])))
    
    # 4. ë¸Œë¦¬í•‘ ìƒì„±
    print("\n[4/5] ìµœì¢… ë¸Œë¦¬í•‘ ìƒì„± ì¤‘...")
    briefing_tool = FinalBriefingTool()
    briefing = briefing_tool._run(str(summaries), str(analysis))
    print(f"   [OK] ë¸Œë¦¬í•‘ ìƒì„± ì™„ë£Œ ({len(briefing)} ì)")
    
    # 5. Notion ì—…ë¡œë“œ
    print("\n[5/5] Notion ì—…ë¡œë“œ ì¤‘...")
    notion_tool = NotionUploadTool()
    result = notion_tool._run(briefing, str(analysis))
    print(f"   {result}")
    
    print("\n[COMPLETE] ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    return briefing

if __name__ == "__main__":
    weekday = datetime.today().weekday()
    
    # í‰ì¼(0-4)ì—ë§Œ ì‹¤í–‰, ì£¼ë§(5-6)ì€ ìŠ¤í‚µ
    if weekday >= 5:
        print(f"[SKIP] ì£¼ë§ ìŠ¤í‚µ - {today_display} ({'í† ìš”ì¼' if weekday == 5 else 'ì¼ìš”ì¼'})")
    else:
        result = run_daily_briefing()
        
        print("\n" + "=" * 60)
        print("ìµœì¢… ë¸Œë¦¬í•‘ ë¯¸ë¦¬ë³´ê¸°:")
        print("=" * 60)
        print(result[:800] + "..." if len(result) > 800 else result)
