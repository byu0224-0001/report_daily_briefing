# ==========================================================
# CrewAI Daily Briefing v9.0 (ì „ìˆ˜ ë¦¬í¬íŠ¸ ë¶„ì„ + í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸)
# - ëª¨ë“  ë¦¬í¬íŠ¸ ì „ìˆ˜ ìš”ì•½ (PDF + HTML)
# - gpt-4o-mini (ì••ì¶•) + gpt-5-mini (ë¸Œë¦¬í•‘)
# - ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ ìµœì í™”
# ==========================================================
import os, re, time, fitz, requests, pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from collections import Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
from openai import OpenAI
from crewai.tools import BaseTool
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# ----------------------------------------------------------
# 0ï¸âƒ£ í™˜ê²½ ì„¤ì •
# ----------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
NOTION_DATABASE_ID = os.getenv("NOTION_DATABASE_ID")

# í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸: gpt-4o-mini (ì••ì¶•) + gpt-5-mini (ë¸Œë¦¬í•‘)
LLM_SUMMARY = "gpt-4o-mini"  # ë¦¬í¬íŠ¸ ìš”ì•½ìš© (ë¹ ë¥´ê³  ì €ë ´)
LLM_BRIEFING = os.getenv("OPENAI_MODEL_NAME", "gpt-5-mini")  # ë¸Œë¦¬í•‘ ìƒì„±ìš©
client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"}
NOTION_HEADERS = {"Authorization": f"Bearer {NOTION_API_KEY}",
                  "Notion-Version": "2022-06-28",
                  "Content-Type": "application/json"}

today_display = datetime.now().strftime("%Y.%m.%d")
today_file = datetime.now().strftime("%Y-%m-%d")

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìµœê·¼ 3ì¼ì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ (ì£¼ë§ ëŒ€ì‘)
TEST_MODE_RECENT_DAYS = False  # True: ìµœê·¼ 3ì¼, False: ì˜¤ëŠ˜ë§Œ (í”„ë¡œë•ì…˜ ëª¨ë“œ)
if TEST_MODE_RECENT_DAYS:
    # 4ìë¦¬ ì—°ë„ì™€ 2ìë¦¬ ì—°ë„ ë‘˜ ë‹¤ ìƒì„±
    target_dates_full = [(datetime.now() - timedelta(days=i)).strftime("%Y.%m.%d") for i in range(3)]
    target_dates_short = [(datetime.now() - timedelta(days=i)).strftime("%y.%m.%d") for i in range(3)]
    target_dates = target_dates_full + target_dates_short  # ë‘˜ ë‹¤ í—ˆìš©
    print(f"[TEST] ìµœê·¼ 3ì¼ì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ëª¨ë“œ: {', '.join(target_dates_full)}")
else:
    target_dates = [today_display, datetime.now().strftime("%y.%m.%d")]
    print(f"[PROD] ì˜¤ëŠ˜ ë‚ ì§œë§Œ ìˆ˜ì§‘: {today_display}")

# ----------------------------------------------------------
# ğŸŒ Selenium ì„¤ì •
# ----------------------------------------------------------
def create_selenium_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument(f"user-agent={HEADERS['User-Agent']}")
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=chrome_options)

# ----------------------------------------------------------
# 1ï¸âƒ£ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
# ----------------------------------------------------------
class NaverResearchScraperTool(BaseTool):
    name: str = "Naver Research Scraper Tool"
    description: str = "ë„¤ì´ë²„ ê¸ˆìœµ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"
    
    def _run(self) -> str:
        """ë„¤ì´ë²„ ë¦¬ì„œì¹˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ (Selenium)"""
        base_url = "https://finance.naver.com/research/"
        categories = {
            "íˆ¬ìì •ë³´": "invest_list.naver",
            "ì¢…ëª©ë¶„ì„": "company_list.naver",
            "ì‚°ì—…ë¶„ì„": "industry_list.naver",
            "ê²½ì œë¶„ì„": "economy_list.naver"
        }
        reports = []
        driver = create_selenium_driver()
        print(f"\n[DEBUG] ë„¤ì´ë²„ ìˆ˜ì§‘ ì‹œì‘ - ë‚ ì§œ: {target_dates}")
        try:
            for cat, path in categories.items():
                url = base_url + path
                print(f"\n[DEBUG] {cat} í˜ì´ì§€ ì ‘ì†: {url}")
                driver.get(url)
                time.sleep(3)  # ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                
                # í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥ (ë””ë²„ê¹…ìš©)
                page_source = driver.page_source
                if "table" not in page_source.lower():
                    print(f"   âš ï¸ {cat}: í˜ì´ì§€ ì†ŒìŠ¤ì— 'table' ì—†ìŒ")
                    continue
                
                soup = BeautifulSoup(page_source, "html.parser")
                
                # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
                rows = soup.select("table.type_1 tbody tr")
                if not rows:
                    rows = soup.select("table tbody tr")
                if not rows:
                    rows = soup.select("tbody tr")
                if not rows:
                    rows = soup.find_all("tr")
                
                print(f"[DEBUG] {cat}: {len(rows)}ê°œ row ë°œê²¬")
                for i, row in enumerate(rows):
                    cols = row.find_all("td")
                    if len(cols) < 4: 
                        continue
                    
                    # ì»¬ëŸ¼ êµ¬ì¡° ë¶„ì„ (ì¢…ëª©ëª…, ì œëª©, ì¦ê¶Œì‚¬, ì²¨ë¶€, ì‘ì„±ì¼, ì¡°íšŒìˆ˜)
                    # ì œëª©ì€ ë³´í†µ cols[0] ë˜ëŠ” cols[1]
                    title_tag = cols[0].find("a")
                    if not title_tag and len(cols) > 1:
                        title_tag = cols[1].find("a")
                    
                    # ì¦ê¶Œì‚¬ëŠ” ë³´í†µ cols[1] ë˜ëŠ” cols[2]
                    company = cols[2].get_text(strip=True) if len(cols) > 2 else "N/A"
                    if not company or company == "":
                        company = cols[1].get_text(strip=True) if len(cols) > 1 else "N/A"
                    
                    # ë‚ ì§œ ì°¾ê¸°: ë’¤ì—ì„œ ë‘ ë²ˆì§¸ ì»¬ëŸ¼ (ì‘ì„±ì¼)
                    date = ""
                    if len(cols) >= 6:  # 6ê°œ ì»¬ëŸ¼: [ì¢…ëª©ëª…, ì œëª©, ì¦ê¶Œì‚¬, ì²¨ë¶€, ì‘ì„±ì¼, ì¡°íšŒìˆ˜]
                        date = cols[4].get_text(strip=True)  # ì‘ì„±ì¼ (5ë²ˆì§¸, 0-indexed)
                    elif len(cols) >= 5:  # 5ê°œ ì»¬ëŸ¼: [ì œëª©, ì¦ê¶Œì‚¬, ì²¨ë¶€, ì‘ì„±ì¼, ì¡°íšŒìˆ˜]
                        date = cols[3].get_text(strip=True)  # ì‘ì„±ì¼
                    else:
                        date = cols[-2].get_text(strip=True)  # ë’¤ì—ì„œ ë‘ ë²ˆì§¸
                    
                    # ë””ë²„ê·¸: ì²˜ìŒ 5ê°œ row ì¶œë ¥
                    if i < 5:
                        title_text = title_tag.get_text(strip=True)[:30] if title_tag else 'N/A'
                        print(f"   - [{date}] {title_text}... (ì»¬ëŸ¼ìˆ˜: {len(cols)})")
                    
                    # ë‚ ì§œ í˜•ì‹ í†µì¼ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
                    date_clean = date.replace(" ", "").replace(".", ".").strip()
                    
                    # ë‚ ì§œ í•„í„°
                    if date_clean not in target_dates:
                        continue
                    if not title_tag:
                        continue
                    
                    # href ì¶”ì¶œ ë° ê²€ì¦
                    href = title_tag.get("href", "")
                    if not href or href == "#":
                        continue
                    
                    # URL ì¡°í•©: ì ˆëŒ€ ê²½ë¡œ ì²´í¬
                    if href.startswith("http"):
                        detail_url = href
                    elif href.startswith("/"):
                        detail_url = "https://finance.naver.com" + href
                    else:
                        detail_url = "https://finance.naver.com/" + href
                    
                    # PDF URL ì¶”ì¶œ: ì²¨ë¶€ ì»¬ëŸ¼ì—ì„œ ì§ì ‘ ì°¾ê¸° (ê°•í™”)
                    pdf_url = None
                    try:
                        # ì²¨ë¶€ ì»¬ëŸ¼ (ë³´í†µ cols[3] ë˜ëŠ” cols[4])
                        attach_col = cols[3] if len(cols) > 3 else cols[2] if len(cols) > 2 else None
                        if attach_col:
                            # ë„¤ì´ë²„ PDF ë‹¤ìš´ë¡œë“œ ë§í¬ íŒ¨í„´ ê°•í™”
                            pdf_link = attach_col.find("a", href=re.compile(r"\.pdf|download|filekey|attach|report|view", re.IGNORECASE))
                            if pdf_link:
                                href = pdf_link.get("href", "")
                                if href.startswith("http"):
                                    pdf_url = href
                                elif href.startswith("/"):
                                    pdf_url = "https://finance.naver.com" + href
                                else:
                                    pdf_url = "https://finance.naver.com/" + href
                        
                        # ìƒì„¸ í˜ì´ì§€ì—ì„œ PDF ì°¾ê¸°
                        if not pdf_url:
                            d_res = requests.get(detail_url, headers=HEADERS, timeout=5)
                            d_soup = BeautifulSoup(d_res.text, "html.parser")
                            
                            # ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„ (ê°•í™”)
                            pdf_btn = d_soup.find("a", href=re.compile(r"download|view|filekey|attach|\.pdf", re.IGNORECASE))
                            if not pdf_btn:
                                pdf_btn = d_soup.find("a", string=re.compile("ë¦¬í¬íŠ¸ë³´ê¸°|PDF|ë‹¤ìš´ë¡œë“œ|ë³´ê¸°", re.IGNORECASE))
                            if not pdf_btn:
                                pdf_btn = d_soup.find("a", class_=re.compile("pdf|download|report", re.IGNORECASE))
                            
                            if pdf_btn:
                                pdf_href = pdf_btn.get("href", "")
                                if pdf_href.startswith("http"):
                                    pdf_url = pdf_href
                                elif pdf_href.startswith("/"):
                                    pdf_url = "https://finance.naver.com" + pdf_href
                                else:
                                    pdf_url = "https://finance.naver.com/" + pdf_href
                    except Exception as e:
                        pdf_url = None
                    # URL ìœ íš¨ì„± ê²€ì‚¬ (ì¢…ëª© í™”ë©´ ë¦¬ë‹¤ì´ë ‰íŠ¸ ì œì™¸ - í™•ì¥)
                    valid_url = detail_url
                    if not detail_url or not detail_url.startswith("http"):
                        valid_url = None
                    # ëª¨ë“  /item/ íŒ¨í„´ ì œì™¸ (ì¢…ëª© í˜ì´ì§€)
                    if valid_url and "/item/" in valid_url:
                        valid_url = None
                    
                    reports.append({
                        "source": "ë„¤ì´ë²„",
                        "category": cat,
                        "title": title_tag.get_text(strip=True),
                        "company": company,
                        "date": date,
                        "url": valid_url,
                        "pdf_url": pdf_url
                    })
                print(f"   [OK] {cat}: {len([r for r in reports if r['category'] == cat])}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                time.sleep(1)
        finally:
            driver.quit()
        print(f"[OK] ë„¤ì´ë²„: {len(reports)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return str(reports)

class HankyungScraperTool(BaseTool):
    name: str = "Hankyung Scraper Tool"
    description: str = "í•œê²½ ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"
    
    def _run(self) -> str:
        """í•œê²½ì»¨ì„¼ì„œìŠ¤ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"""
        url = "https://consensus.hankyung.com/analysis/list"
        reports = []
        print(f"[DEBUG] í•œê²½ ìˆ˜ì§‘ ì‹œì‘ - ê²€ìƒ‰ ë‚ ì§œ: {target_dates[:3]}")
        try:
            res = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")
            rows = soup.select("table tbody tr")
            print(f"[DEBUG] í•œê²½: {len(rows)}ê°œ row ë°œê²¬")
            row_count = 0
            collected_count = 0
            for row in rows:
                cols = row.find_all("td")
                if len(cols) < 4: 
                    continue
                date_raw = cols[3].get_text(strip=True)
                # ë‚ ì§œ í˜•ì‹ í†µì¼ (YYYY-MM-DD â†’ YYYY.MM.DD, YY-MM-DD â†’ YY.MM.DD)
                date = date_raw.replace("-", ".")
                # ë‚ ì§œ í•„í„°: target_dates ëª©ë¡ì— ìˆëŠ” ë‚ ì§œë§Œ ìˆ˜ì§‘
                if date not in target_dates:
                    continue
                title_tag = cols[0].find("a")
                if not title_tag:
                    continue
                pdf_tag = row.find("a", href=re.compile(r"\.pdf$"))
                pdf_url = "https://consensus.hankyung.com" + pdf_tag["href"] if pdf_tag else None
                reports.append({
                    "source": "í•œê²½ì»¨ì„¼ì„œìŠ¤",
                    "category": cols[2].get_text(strip=True),
                    "title": title_tag.get_text(strip=True),
                    "company": cols[1].get_text(strip=True),
                    "date": date,
                    "url": "https://consensus.hankyung.com" + title_tag["href"],
                    "pdf_url": pdf_url
                })
                collected_count += 1
                time.sleep(0.5)
            print(f"   [OK] í•œê²½: {collected_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ í•œê²½ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return str(reports)

# ----------------------------------------------------------
# 2ï¸âƒ£ í‚¤ì›Œë“œ ë¶„ì„ (ë‚ ì§œ ì œì™¸)
# ----------------------------------------------------------
class PythonAnalyzerTool(BaseTool):
    name: str = "Python Analyzer Tool"
    description: str = "ë¦¬í¬íŠ¸ ì œëª© ê¸°ë°˜ í‚¤ì›Œë“œ/ì¹´í…Œê³ ë¦¬ ë¶„ì„"
    
    def _run(self, reports_str: str) -> str:
        """í‚¤ì›Œë“œ ë° ì¹´í…Œê³ ë¦¬ ë¶„ì„"""
        try:
            reports = eval(reports_str)
            df = pd.DataFrame(reports).drop_duplicates(subset=["title", "company"])
            
            # ë‹¨ì–´ ì¶”ì¶œ
            words = sum([re.findall(r"[ê°€-í£A-Za-z0-9]{2,12}", t) for t in df["title"]], [])
            
            # ë‚ ì§œ ê´€ë ¨ í‚¤ì›Œë“œ ì œì™¸ (10, 24, 26, 10ì›”, 2025 ë“±)
            today = datetime.now().day
            month = datetime.now().month
            year = datetime.now().year
            date_pattern = re.compile(r"(\d{1,2}ì›”|\d{1,2}ì¼|20\d{2}|\d{2}\.\d{2}|\d{4}\.\d{2}\.\d{2})")
            
            # ìˆ«ì ì „ìš© íŒ¨í„´ (ëª¨ë“  ìˆ«ì ì œì™¸)
            number_pattern = re.compile(r'^\d+$')
            
            stop_words = {
                "ë¦¬í¬íŠ¸", "ë¶„ì„", "ì „ë§", "íˆ¬ì", "ê²½ì œ", "ì‚°ì—…", "ì´ìŠˆ",
                str(today), str(month), str(year), f"{month}ì›”", "2025", "25", "24", "10", "26",
                "Weekly", "Preview", "Monitor", "Daily", "ì£¼ê°„", "ì£¼ì°¨", "ì¼ë³´",
                "China", "Weekly", "3Q25", "4ì£¼ì°¨", "10ì›”", "11ì›”", "12ì›”"
            }
            
            filtered_words = [
                w for w in words
                if w not in stop_words 
                and not date_pattern.search(w) 
                and not number_pattern.match(w)  # ìˆœìˆ˜ ìˆ«ì ì œì™¸
                and len(w) >= 2
                and w.isalnum()  # ì˜ë¬¸ì/í•œê¸€ë§Œ í—ˆìš©
            ]
            
            counter = Counter(filtered_words)
            
            result = {
                "total_reports": len(df),
                "top_keywords": ", ".join([f"{k}({v}íšŒ)" for k, v in counter.most_common(10)]),
                "category_summary": df["category"].value_counts().to_dict(),
                "reports": df.to_dict("records")  # ë¦¬í¬íŠ¸ ì „ì²´ ì •ë³´ í¬í•¨
            }
            return str(result)
        except Exception as e:
            return f"ë¶„ì„ ì‹¤íŒ¨: {e}"

# ----------------------------------------------------------
# 3ï¸âƒ£ ê° ë¦¬í¬íŠ¸ë³„ í•µì‹¬ 1ì¤„ ìš”ì•½ (PDF ë‚´ìš© í¬í•¨)
# ----------------------------------------------------------
class ReportSummarizerTool(BaseTool):
    name: str = "Report Summarizer Tool"
    description: str = "ì „ì²´ ë¦¬í¬íŠ¸ ì „ìˆ˜ ìš”ì•½ (ë³‘ë ¬ ì²˜ë¦¬)"
    
    def _extract_pdf_text(self, pdf_url: str) -> str:
        """PDF ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            # PDF URL ìœ íš¨ì„± ê²€ì¦
            if not pdf_url or not isinstance(pdf_url, str):
                return ""
            
            # URL íŒŒë¼ë¯¸í„° ì œê±° (query string, fragment ì œê±°)
            original_url = pdf_url
            pdf_url = pdf_url.split("?")[0].split("#")[0]
            
            # ê³µë°± ì œê±° ë¨¼ì € (ì¤‘ìš”!)
            pdf_url_stripped = pdf_url.strip()
            
            # PDF URL ë³´ì • (`.p` â†’ `.pdf` ìë™ ì¶”ê°€) - ê°œì„  ë²„ì „
            if pdf_url_stripped and not pdf_url_stripped.lower().endswith(".pdf"):
                # URLì´ ì˜ë¦° ê²½ìš° (.p ë˜ëŠ” .pdë¡œ ëë‚˜ëŠ” ê²½ìš°)
                if pdf_url_stripped.endswith(".p"):
                    pdf_url = pdf_url_stripped[:-1] + "pdf"  # .p â†’ .pdf (ë²„ê·¸ ìˆ˜ì •)
                elif pdf_url_stripped.endswith(".pd"):
                    pdf_url = pdf_url_stripped[:-2] + "pdf"  # .pd â†’ .pdf
                # pstatic URL íŒ¨í„´ íŠ¹ë³„ ì²˜ë¦¬ (ì ìœ¼ë¡œ ëë‚˜ì§€ ì•ŠëŠ” ê²½ìš°ë§Œ)
                elif "pstatic.net" in pdf_url_stripped and not pdf_url_stripped.endswith("."):
                    pdf_url = pdf_url_stripped + ".pdf"
                # ìˆ«ìë¡œ ëë‚˜ëŠ” URLì—ë„ .pdf ìë™ ì¶”ê°€
                elif re.search(r'/\d+$', pdf_url_stripped):
                    pdf_url = pdf_url_stripped + ".pdf"
                else:
                    pdf_url = pdf_url_stripped
            else:
                pdf_url = pdf_url_stripped
            
            print(f"      [DEBUG PDF] ì›ë³¸: {original_url[:80]} â†’ ìˆ˜ì •: {pdf_url[:80]}")
            
            # HTTP ìš”ì²­ ì‹œë„ (ë‹¤ì¤‘ fallback)
            res = None
            attempts = []
            
            # 1. ë³´ì •ëœ URLë¶€í„° ì‹œë„
            if pdf_url_stripped != pdf_url:
                attempts.append(pdf_url)
                print(f"      [DEBUG PDF] ë³´ì • URL ì¶”ê°€: {pdf_url[:80]}")
            
            # 2. ì›ë³¸ URL ì‹œë„
            attempts.append(pdf_url_stripped)
            
            # 3. .p/.pd íŒ¨í„´ì´ë©´ ìˆ˜ì •ë³¸ ì¶”ê°€ ì‹œë„
            if pdf_url_stripped.endswith(".p") and pdf_url_stripped not in attempts:
                fixed_p = pdf_url_stripped[:-1] + "pdf"  # ë²„ê·¸ ìˆ˜ì •: df â†’ pdf
                attempts.append(fixed_p)
                print(f"      [DEBUG PDF] .p ìˆ˜ì •ë³¸ ì¶”ê°€: {fixed_p[:80]}")
            elif pdf_url_stripped.endswith(".pd") and pdf_url_stripped not in attempts:
                fixed_pd = pdf_url_stripped[:-2] + "pdf"
                attempts.append(fixed_pd)
                print(f"      [DEBUG PDF] .pd ìˆ˜ì •ë³¸ ì¶”ê°€: {fixed_pd[:80]}")
            
            # 4. í™•ì¥ì ì—†ëŠ” ê²½ìš° .pdf ì¶”ê°€ ì‹œë„
            if not pdf_url_stripped.endswith(".pdf") and not pdf_url_stripped.endswith(".p") and not pdf_url_stripped.endswith(".pd"):
                attempts.append(pdf_url_stripped + ".pdf")
                print(f"      [DEBUG PDF] .pdf ì¶”ê°€ ì‹œë„: {(pdf_url_stripped + '.pdf')[:80]}")
            
            for i, attempt_url in enumerate(attempts):
                try:
                    print(f"      [DEBUG PDF] ì‹œë„ {i+1}/{len(attempts)}: {attempt_url[:80]}")
                    res = requests.get(attempt_url, headers=HEADERS, timeout=15, stream=True)
                    if res.status_code == 200:
                        pdf_url = attempt_url
                        if len(attempts) > 1:
                            print(f"      [DEBUG PDF] âœ“ ì„±ê³µ! {attempt_url[:80]}")
                        break
                    else:
                        print(f"      [DEBUG PDF] HTTP {res.status_code}")
                except Exception as e:
                    print(f"      [DEBUG PDF] ì˜ˆì™¸: {str(e)[:50]}")
                    continue
            
            if not res or res.status_code != 200:
                print(f"      [DEBUG PDF] HTTP {res.status_code if res else 'None'} - ëª¨ë“  ì‹œë„ ì‹¤íŒ¨")
                return ""
            
            # Content-Type ê²€ì¦ ì™„í™” (PDFê°€ ì•„ë‹ˆì–´ë„ ì‹œë„)
            content_type = res.headers.get('content-type', '').lower()
            if 'pdf' not in content_type and not pdf_url.endswith('.pdf'):
                print(f"      [DEBUG PDF] Content-Type: {content_type} (PDF ì•„ë‹˜)")
                
                # HTML ì‘ë‹µì¸ ê²½ìš° ì¬ì‹œë„
                if content_type.startswith('text/html'):
                    print(f"      [DEBUG PDF] HTML ì‘ë‹µ â†’ URL ì¬êµ¬ì„± ì‹œë„")
                    # .pdf ìë™ ì¶”ê°€ ì‹œë„
                    alt_pdf = pdf_url.split("?")[0] + ".pdf"
                    try:
                        res_alt = requests.get(alt_pdf, headers=HEADERS, timeout=10)
                        if res_alt.status_code == 200 and 'pdf' in res_alt.headers.get('content-type', '').lower():
                            res = res_alt
                            pdf_url = alt_pdf
                            print(f"      [DEBUG PDF] ì¬êµ¬ì„± ì„±ê³µ: {alt_pdf[:80]}")
                        else:
                            print(f"      [DEBUG PDF] HTML ì¬ì‹œë„ ì‹¤íŒ¨ â†’ PDF ì•„ë‹˜")
                            return ""
                    except Exception as e:
                        print(f"      [DEBUG PDF] HTML ì¬ì‹œë„ ì˜ˆì™¸: {str(e)[:50]}")
                        return ""
            
            # íŒŒì¼ëª…ì„ ê³ ìœ í•˜ê²Œ ìƒì„± (ë™ì‹œ ì ‘ê·¼ ë°©ì§€)
            import uuid
            temp_file = f"temp_{uuid.uuid4().hex[:8]}.pdf"
            
            try:
                with open(temp_file, "wb") as f:
                    f.write(res.content)
                
                with fitz.open(temp_file) as pdf:
                    text = ""
                    total = len(pdf)
                    pages = list(range(min(5, total))) + list(range(max(0, total - 3), total))
                    for p in sorted(set(pages)):
                        text += pdf[p].get_text()
                
                print(f"      [DEBUG PDF] ì¶”ì¶œ ì„±ê³µ: {len(text)}ì")
                
                # íŒŒì¼ ë‹«íŒ í›„ ì‚­ì œ
                import time
                time.sleep(0.1)  # íŒŒì¼ í•¸ë“¤ í•´ì œ ëŒ€ê¸°
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                
                return re.sub(r"\s+", " ", text.strip())[:3500]
            except Exception as pdf_error:
                print(f"      [DEBUG PDF] íŒŒì‹± ì‹¤íŒ¨: {pdf_error}")
                return ""
        except Exception as e:
            print(f"      [PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}]")
            return ""
    
    def _extract_html_text(self, url: str) -> str:
        """PDFê°€ ì—†ì„ ê²½ìš° HTML ë³¸ë¬¸ í¬ë¡¤ë§ (Seleniumìœ¼ë¡œ JS ë Œë”ë§ëœ í˜ì´ì§€)"""
        try:
            print(f"      [DEBUG HTML] URL: {url[:80]}")
            # Seleniumìœ¼ë¡œ JS ë Œë”ë§ëœ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
            driver = create_selenium_driver()
            driver.get(url)
            time.sleep(3)  # JS ë¡œë”© ëŒ€ê¸°
            
            # iframeì´ ìˆëŠ” ê²½ìš° ë‚´ë¶€ ë¬¸ì„œ ì ‘ê·¼ (ì´ì¤‘ iframe íƒìƒ‰)
            try:
                iframes = driver.find_elements("tag name", "iframe")
                if iframes:
                    driver.switch_to.frame(iframes[0])
                    time.sleep(1)
                    # ì´ì¤‘ iframe í™•ì¸
                    inner_iframes = driver.find_elements("tag name", "iframe")
                    if inner_iframes:
                        driver.switch_to.frame(inner_iframes[0])
                        time.sleep(1)
                    html = driver.page_source
                    driver.switch_to.default_content()
                else:
                    html = driver.page_source
            except Exception:
                # iframe ì „í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í˜ì´ì§€ ì‚¬ìš©
                html = driver.page_source
            
            driver.quit()
            
            soup = BeautifulSoup(html, "html.parser")
            
            # ë„¤ì´ë²„ ë¦¬í¬íŠ¸ í˜ì´ì§€ êµ¬ì¡°ì— ë§ì¶° ë³¸ë¬¸ ì¶”ì¶œ
            # ì£¼ìš” ì„¹ì…˜ ì„ íƒìë“¤ (td ìš°ì„ ìˆœìœ„ ìƒí–¥)
            content_selectors = [
                "td.view_content",     # í…Œì´ë¸” ì…€ ë³¸ë¬¸ (ê²½ì œ/ì‚°ì—… ë¶„ì„ ìš°ì„ )
                "table.view",          # í…Œì´ë¸” ë·°
                "div.view_con",        # ë„¤ì´ë²„ ë¦¬í¬íŠ¸ ë³¸ë¬¸
                "div.tb_view",         # í…Œì´ë¸” í˜•ì‹ ì¶”ê°€
                "div.article_view", 
                "div.article_view_con",
                "div.tb_type1",        # í…Œì´ë¸” í˜•ì‹
                "div.tb_cont",         # í…Œì´ë¸” ì»¨í…ì¸ 
                "div.board_view",      # ê²Œì‹œíŒ í˜•ì‹
                "article",
                "div.content",
                "#content"
            ]
            
            text = ""
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(separator="\n").strip()
                    print(f"      [DEBUG HTML] ì„ íƒì '{selector}'ë¡œ {len(text)}ì ì¶”ì¶œ")
                    break
            
            # ìœ„ ì„ íƒìë¡œ ëª» ì°¾ìœ¼ë©´ ì „ì²´ ë³¸ë¬¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
            if not text or len(text) < 100:  # 200ì â†’ 100ìë¡œ ì™„í™”
                print(f"      [DEBUG HTML] ì„ íƒìë¡œ ì¶”ì¶œ ì‹¤íŒ¨, fallback ì‹œë„")
                # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
                for tag in soup(["script", "style", "nav", "footer", "header"]):
                    tag.decompose()
                text = soup.get_text(separator="\n").strip()
                
                # ì—¬ì „íˆ ì§§ìœ¼ë©´ p, td, div íƒœê·¸ì—ì„œ ê¸´ í…ìŠ¤íŠ¸ ì°¾ê¸°
                if not text or len(text) < 100:  # 200ì â†’ 100ìë¡œ ì™„í™”
                    for tag in soup.find_all(["p", "td", "div"]):
                        tag_text = tag.get_text(separator=" ").strip()
                        if len(tag_text) > 100 and not re.search(r"ë„¤ì´ë²„|ì‚­ì œ|ì˜¤ë¥˜|ì£¼ì‹ê±°ë˜", tag_text, re.IGNORECASE):
                            text = tag_text
                            print(f"      [DEBUG HTML] fallbackìœ¼ë¡œ {len(text)}ì ì¶”ì¶œ")
                            break
            
            # ê´‘ê³ /ë„¤ë¹„ê²Œì´ì…˜ í…ìŠ¤íŠ¸ í•„í„°ë§
            text = re.sub(r"\s+", " ", text.strip())
            
            # 404 ì—ëŸ¬ í˜ì´ì§€ ì²´í¬
            error_patterns = [
                r"ë°©ë¬¸í•˜ì‹œë ¤ëŠ” í˜ì´ì§€ì˜ ì£¼ì†Œê°€ ì˜ëª»",
                r"í˜ì´ì§€ì˜ ì£¼ì†Œê°€ ë³€ê²½",
                r"ì‚­ì œë˜ì—ˆê±°ë‚˜",
                r"ë„¤ì´ë²„ :: ì„¸ìƒì˜ ëª¨ë“  ì§€ì‹",
            ]
            
            for pattern in error_patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    return ""  # ì—ëŸ¬ í˜ì´ì§€ëŠ” ë¹ˆ í…ìŠ¤íŠ¸ ë°˜í™˜
            
            # ìœ íš¨í•œ ë³¸ë¬¸ì¸ì§€ íŒë‹¨ (ìµœì†Œ 50ì ì´ìƒìœ¼ë¡œ ì™„í™”)
            if len(text) < 50:
                print(f"      [DEBUG HTML] ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {len(text)}ì")
                return ""
            
            # ê´‘ê³  íŒ¨í„´ ì²´í¬ (ë” ì •êµí•˜ê²Œ)
            ad_patterns = [
                r"ë„¤ì´ë²„ ì£¼ì‹ê±°ë˜ì—°ê²°.*ë¹ ë¥¸ ì£¼ë¬¸.*ë„ì™€ë“œë¦½ë‹ˆë‹¤",  # ì—°ê²°ëœ ê´‘ê³  í…ìŠ¤íŠ¸
                r"^.{0,100}ì£¼ì„.*ê²°ë¡ .*ì°¸ê³ .*$",  # ë„ˆë¬´ ì§§ì€ ë°˜ë³µ íŒ¨í„´
            ]
            
            for pattern in ad_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
                if matches and len(text) < 500:
                    # ê´‘ê³  í…ìŠ¤íŠ¸ê°€ ì£¼ìš” ë‚´ìš©ì´ê³  ì „ì²´ê°€ ì§§ìœ¼ë©´ ì œì™¸
                    return ""
            
            # ìµœì¢… ì •ì œ ë° ê¸¸ì´ ì œí•œ (3500ìë¡œ í™•ì¥)
            text = text[:3500]
            print(f"      [DEBUG HTML] ìµœì¢… ì¶”ì¶œ ì„±ê³µ: {len(text)}ì")
            return text
        except Exception as e:
            print(f"      [HTML ì¶”ì¶œ ì‹¤íŒ¨: {e}]")
            import traceback
            traceback.print_exc()
            return ""
    
    def _summarize_report(self, report: dict, idx: int, total: int) -> dict:
        """ë‹¨ì¼ ë¦¬í¬íŠ¸ ìš”ì•½ (gpt-4o-mini ì‚¬ìš©)"""
        title = report["title"]
        company = report["company"]
        category = report["category"]
        pdf_url = report.get("pdf_url")
        url = report.get("url")
        
        # PDF ë˜ëŠ” HTML í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = ""
        source_type = "ì—†ìŒ"
        
        if pdf_url:
            text = self._extract_pdf_text(pdf_url)
            if text:
                source_type = "PDF"
        
        if not text and url:
            text = self._extract_html_text(url)
            if text:
                source_type = "HTML"
        
        # GPT ìš”ì•½ (gpt-4o-mini)
        text_preview = text[:2000] if text else '[ë³¸ë¬¸ ì—†ìŒ]'
        
        # ë””ë²„ê·¸ ë¡œê·¸ (ìƒì„¸) - ASCIIë¡œë§Œ ì¶œë ¥
        if text:
            text_safe = text[:60].encode('ascii', 'ignore').decode('ascii')
            print(f"      [{source_type}] {len(text)}ì: {text_safe}...")
        else:
            title_safe = title[:40].encode('ascii', 'ignore').decode('ascii')
            print(f"\n[DIAG] {title_safe}")
            if pdf_url:
                print(f"   -> PDF URL: {pdf_url[:80]}")
            if url:
                print(f"   -> HTML URL: {url[:80]}")
            print(f"   -> ì›ì¸: PDFì™€ HTML ë‘˜ ë‹¤ ì‹œë„í–ˆìœ¼ë‚˜ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
        
        # ë³¸ë¬¸ ì—†ìœ¼ë©´ ì œëª©ê³¼ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ìœ¼ë¡œë§Œ ìš”ì•½
        if not text:
            prompt = f"""ì•„ë˜ ë¦¬í¬íŠ¸ ì œëª©ê³¼ ì¹´í…Œê³ ë¦¬ë§Œ ë³´ê³  í•µì‹¬ì„ 1ë¬¸ì¥ìœ¼ë¡œ ì¶”ì •í•˜ë¼.
ì œëª©: {title}
ì¦ê¶Œì‚¬: {company}
ì¹´í…Œê³ ë¦¬: {category}
ìš”ì•½ (ìœ ì¶”ëœ ì£¼ìš” ë‚´ìš© 1ë¬¸ì¥):"""
        else:
            prompt = f"""ì•„ë˜ ë¦¬í¬íŠ¸ë¥¼ ì½ê³  í•µì‹¬ë§Œ 1ë¬¸ì¥ìœ¼ë¡œ ì••ì¶•í•˜ë¼.
ì œëª©: {title}
ì¦ê¶Œì‚¬: {company}
ë³¸ë¬¸: {text_preview}
ìš”ì•½ (ê¸°ì—…ëª…+íˆ¬ìí¬ì¸íŠ¸ í¬í•¨):"""
        try:
            resp = client.chat.completions.create(
                model=LLM_SUMMARY,
                messages=[
                    {"role": "system", "content": "í•µì‹¬ë§Œ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½"},
                    {"role": "user", "content": prompt}
                ]
            )
            summary = resp.choices[0].message.content.strip()
        except Exception as e:
            summary = f"[ìš”ì•½ ì‹¤íŒ¨: {e}]"
        
        title_safe = title[:35].encode('ascii', 'ignore').decode('ascii')
        company_safe = company.encode('ascii', 'ignore').decode('ascii')
        print(f"[OK] [{idx+1}/{total}] {title_safe}... ({company_safe})")
        return {"title": title, "company": company, "category": category, "summary": summary}
    
    def _run(self, reports_str: str) -> str:
        """ì „ì²´ ë¦¬í¬íŠ¸ ì „ìˆ˜ ìš”ì•½ (ë³‘ë ¬ ì²˜ë¦¬)"""
        reports = eval(reports_str)
        total_reports = len(reports)
        print(f"\n[INFO] ì´ {total_reports}ê°œ ë¦¬í¬íŠ¸ ì „ìˆ˜ ìš”ì•½ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)")
        
        summaries = []
        # ë³‘ë ¬ ì‹¤í–‰ (ìµœëŒ€ 8ê°œ ìŠ¤ë ˆë“œ)
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = {
                executor.submit(self._summarize_report, r, i, total_reports): i
                for i, r in enumerate(reports)
            }
            for future in as_completed(futures):
                summaries.append(future.result())
                time.sleep(0.1)
        
        print(f"\n[OK] ì´ {len(summaries)}ê°œ ë¦¬í¬íŠ¸ ìš”ì•½ ì™„ë£Œ")
        return str(summaries)

# ----------------------------------------------------------
# 4ï¸âƒ£ ìµœì¢… ë¸Œë¦¬í•‘
# ----------------------------------------------------------
class FinalBriefingTool(BaseTool):
    name: str = "Final Briefing Tool"
    description: str = "ê° ë¦¬í¬íŠ¸ ìš”ì•½ì„ ì¢…í•©í•´ íˆ¬ì ë¸Œë¦¬í•‘ ì‘ì„±"
    
    def _run(self, summaries_str: str, analysis_str: str) -> str:
        """ìµœì¢… ë¸Œë¦¬í•‘ ìƒì„±"""
        summaries = eval(summaries_str)
        analysis = eval(analysis_str)
        
        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¦¬í¬íŠ¸ ê·¸ë£¹í™”
        by_category = {}
        for s in summaries:
            cat = s.get('category', 'ê¸°íƒ€')
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(s)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½ ì •ë¦¬
        category_summaries = []
        for cat in ['íˆ¬ìì •ë³´', 'ì¢…ëª©ë¶„ì„', 'ì‚°ì—…ë¶„ì„', 'ê²½ì œë¶„ì„']:
            if cat in by_category:
                reports = by_category[cat]
                summary_texts = [f"- {r['summary']} ({r['company']})" for r in reports]
                category_summaries.append(f"\n### {cat} ({len(reports)}ê±´)\n" + "\n".join(summary_texts))
        
        prompt = f"""ìœ„ ë¦¬í¬íŠ¸ë“¤ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ì„í•˜ì—¬ íˆ¬ìììš© ì¼ì¼ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ë¼.

**ì¤‘ìš”: ëª¨ë“  ë¦¬í¬íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ë¹ ì§ì—†ì´ í¬í•¨í•´ì•¼ í•¨. í•˜ë‚˜ì˜ ë¦¬í¬íŠ¸ë¼ë„ ë†“ì¹˜ë©´ ì•ˆ ë¨.**

{''.join(category_summaries)}

[í‚¤ì›Œë“œ ë¶„ì„]
{analysis.get("top_keywords", "N/A")}

---
**ì‘ë‹µ í˜•ì‹:**

## 1. ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
**ê° ë¦¬í¬íŠ¸ë³„ë¡œ ëª…ì‹œ:**
- ì–´ë–¤ ê¸°ì—…/ì‚°ì—…ì„ ë‹¤ë£¨ëŠ”ê°€
- ë¦¬í¬íŠ¸ì˜ í•µì‹¬ í‰ê°€/ì „ë§ì€ ë¬´ì—‡ì¸ê°€
- íˆ¬ì ì˜ê²¬ì€ ë¬´ì—‡ì¸ê°€ (ëª©í‘œê°€ ìƒí–¥/í•˜í–¥, ë§¤ìˆ˜/ì¤‘ë¦½/ë§¤ë„)

1) íˆ¬ìì •ë³´ ë¦¬í¬íŠ¸ ìš”ì•½ (ëª¨ë“  ë¦¬í¬íŠ¸ í¬í•¨)
2) ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ìš”ì•½ (ëª¨ë“  ë¦¬í¬íŠ¸ í¬í•¨)
3) ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ìš”ì•½ (ëª¨ë“  ë¦¬í¬íŠ¸ í¬í•¨)
4) ê²½ì œë¶„ì„ ë¦¬í¬íŠ¸ ìš”ì•½ (ëª¨ë“  ë¦¬í¬íŠ¸ í¬í•¨)

## 2. í•µì‹¬ í…Œë§ˆ (5~8ê°œ ì¢…ëª©/ì‚°ì—…)
êµ¬ì²´ì  ë‚´ìš© ë°˜ì˜

## 3. íˆ¬ì ì‹œì‚¬ì  (3~4ì¤„)
ê° í…Œë§ˆë³„ íˆ¬ì í¬ì¸íŠ¸, ë¦¬ìŠ¤í¬, ê¸°íšŒ

## 4. ì£¼ëª© í¬ì¸íŠ¸ (3~5ê°œ)
ì‹¤ì  ë°œí‘œì¼ì •, ìˆ˜ì£¼ ê³µì‹œ, ì£¼ìš” ì§€í‘œ ë°œí‘œì¼ ë“±

**ì ˆëŒ€ ê¸ˆì§€:**
- "ì›í•˜ì‹œë©´", "ì¶”ê°€ë¡œ ì œê³µ" ê°™ì€ ì§ˆë¬¸
- ë§ˆë¬´ë¦¬ ë¬¸êµ¬
- ë¦¬í¬íŠ¸ ì¼ë¶€ë§Œ ì–¸ê¸‰í•˜ê³  ìƒëµí•˜ëŠ” ê²ƒ
ìœ„ í˜•ì‹ë§Œ ì‘ì„±"""
        
        try:
            resp = client.chat.completions.create(
                model=LLM_BRIEFING,
                messages=[
                    {"role": "system", "content": "20ë…„ì°¨ ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸. ê° ë¦¬í¬íŠ¸ì˜ í•µì‹¬ì„ ì •í™•íˆ ì¶”ì¶œí•˜ì—¬ ì‹¤ë¬´ íˆ¬ìì ê´€ì ì—ì„œ ì‹¤ì§ˆì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ë¸Œë¦¬í•‘ ì‘ì„±. ë¶ˆí•„ìš”í•œ ì§ˆë¬¸ì´ë‚˜ ë§ˆë¬´ë¦¬ ë¬¸êµ¬ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ì•ŠìŒ."},
                    {"role": "user", "content": prompt}
                ]
            )
            body = resp.choices[0].message.content.strip()
        except Exception as e:
            body = f"[ë¸Œë¦¬í•‘ ìƒì„± ì‹¤íŒ¨: {e}]"
        
        header = f"# {today_file} ì¼ì¼ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë¸Œë¦¬í•‘\n\n*ì´ {analysis['total_reports']}ê±´ ê¸°ë°˜ / {today_display} ë°œí–‰*\n\n"
        return header + body

# ----------------------------------------------------------
# 5ï¸âƒ£ Notion ì—…ë¡œë“œ
# ----------------------------------------------------------
class NotionUploadTool(BaseTool):
    name: str = "Notion Upload Tool"
    description: str = "ìµœì¢… ë¸Œë¦¬í•‘ê³¼ ë¶„ì„ê²°ê³¼ë¥¼ Notion DBì— ì—…ë¡œë“œ"
    
    def _run(self, briefing_text: str, analysis_str: str) -> str:
        """Notionì— ì—…ë¡œë“œ"""
        try:
            analysis = eval(analysis_str)
            total_reports = analysis.get("total_reports", 0)
            
            page_data = {
                "parent": {"database_id": NOTION_DATABASE_ID},
                "properties": {
                    "Name": {"title": [{"text": {"content": f"{today_file} ì¼ì¼ ë¸Œë¦¬í•‘"}}]},
                    "Date": {"date": {"start": today_file}},
                    "ì´ ë¦¬í¬íŠ¸ ìˆ˜": {"number": total_reports},
                    "Top Keywords": {"rich_text": [{"text": {"content": analysis.get("top_keywords", "")[:2000]}}]},
                    "Category Summary": {"rich_text": [{"text": {"content": str(analysis.get("category_summary", {}))[:2000]}}]},
                },
                "children": []
            }
            
            # ë¸Œë¦¬í•‘ ë³¸ë¬¸ì„ childrenìœ¼ë¡œ ì¶”ê°€
            for i in range(0, len(briefing_text), 1800):
                page_data["children"].append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": briefing_text[i:i+1800]}}]
                    }
                })
            
            res = requests.post("https://api.notion.com/v1/pages", headers=NOTION_HEADERS, json=page_data)
            if not res.ok:
                return f"âš ï¸ Notion ì—…ë¡œë“œ ì‹¤íŒ¨: {res.status_code} - {res.text}"
            res.raise_for_status()
            parent_id = res.json().get("id", "")
            return f"[OK] Notion ì—…ë¡œë“œ ì™„ë£Œ (Page ID: {parent_id})"
        except Exception as e:
            return f"âš ï¸ Notion ì—…ë¡œë“œ ì‹¤íŒ¨: {e}"

# ----------------------------------------------------------
# 6ï¸âƒ£ ì‹¤í–‰
# ----------------------------------------------------------
def run_daily_briefing():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print(f"[START] {today_display} Daily Briefing ì‹œì‘ (v9.0 - ì „ìˆ˜ ë¶„ì„)")
    
    # 1. ë¦¬í¬íŠ¸ ìˆ˜ì§‘
    print("\n[1/5] ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì¤‘...")
    naver_tool = NaverResearchScraperTool()
    hankyung_tool = HankyungScraperTool()
    naver_reports = eval(naver_tool._run())
    hankyung_reports = eval(hankyung_tool._run())
    all_reports = naver_reports + hankyung_reports
    
    if len(all_reports) == 0:
        print("[INFO] ë¦¬í¬íŠ¸ ì—†ìŒ")
        return "[INFO] ì—†ìŒ"
    
    print(f"\n[OK] ì´ {len(all_reports)}ê°œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ\n")
    
    # 2. ë¶„ì„
    print("[2/5] í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
    analyzer = PythonAnalyzerTool()
    analysis = eval(analyzer._run(str(all_reports)))
    print(f"   [OK] í‚¤ì›Œë“œ: {analysis['top_keywords'][:100]}...")
    
    # 3. ë¦¬í¬íŠ¸ë³„ ìš”ì•½
    print("\n[3/5] ë¦¬í¬íŠ¸ ìš”ì•½ ì¤‘...")
    summarizer = ReportSummarizerTool()
    summaries = eval(summarizer._run(str(analysis["reports"])))
    
    # 4. ë¸Œë¦¬í•‘ ìƒì„±
    print("\n[4/5] ìµœì¢… ë¸Œë¦¬í•‘ ìƒì„± ì¤‘...")
    briefing_tool = FinalBriefingTool()
    briefing = briefing_tool._run(str(summaries), str(analysis))
    print(f"   [OK] ë¸Œë¦¬í•‘ ìƒì„± ì™„ë£Œ ({len(briefing)} ì)")
    
    # 5. Notion ì—…ë¡œë“œ
    print("\n[5/5] Notion ì—…ë¡œë“œ ì¤‘...")
    notion_tool = NotionUploadTool()
    result = notion_tool._run(briefing, str(analysis))
    print(f"   {result}")
    
    print("\n[COMPLETE] ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    return briefing

if __name__ == "__main__":
    weekday = datetime.today().weekday()
    IS_TEST_MODE = False  # í”„ë¡œë•ì…˜ ëª¨ë“œ (ì£¼ë§ ìŠ¤í‚µ)
    
    if weekday >= 5 and not IS_TEST_MODE:
        print(f"[SKIP] ì£¼ë§ ìŠ¤í‚µ ({today_display})")
    else:
        if IS_TEST_MODE and weekday >= 5:
            print(f"\n[TEST MODE] ì£¼ë§ì´ì§€ë§Œ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.\nì˜¤ëŠ˜: {today_display}")
        
        result = run_daily_briefing()
        
        print("\n" + "=" * 60)
        print("ìµœì¢… ë¸Œë¦¬í•‘ ë¯¸ë¦¬ë³´ê¸°:")
        print("=" * 60)
        print(result[:800] + "..." if len(result) > 800 else result)
