# ==========================================================
# v7-Final: CrewAI Daily Briefing (ë¸Œë¦¬í•‘ ë³µì› + Notion ìë™ ì—…ë¡œë“œ)
# ==========================================================
import os, re, time, fitz, requests, pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from collections import Counter
from typing import List, Dict, Any
from dotenv import load_dotenv
from openai import OpenAI
from crewai import Agent, Task, Crew, Process
from crewai_tools import BaseTool, tool

# ----------------------------------------------------------
# 0ï¸âƒ£ í™˜ê²½ ì„¤ì •
# ----------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
NOTION_DATABASE_ID = os.getenv("NOTION_DATABASE_ID")
LLM_MODEL = os.getenv("OPENAI_MODEL_NAME", "gpt-5-mini")
client = OpenAI(api_key=OPENAI_API_KEY)

HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"}
NOTION_HEADERS = {"Authorization": f"Bearer {NOTION_API_KEY}",
                  "Notion-Version": "2022-06-28",
                  "Content-Type": "application/json"}

today_display = datetime.now().strftime("%Y.%m.%d")
today_file = datetime.now().strftime("%Y-%m-%d")

# ----------------------------------------------------------
# 1ï¸âƒ£ ë„¤ì´ë²„ / í•œê²½ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
# ----------------------------------------------------------
@tool("Naver Research Scraper")
def naver_research_scraper() -> List[Dict[str, str]]:
    base_url = "https://finance.naver.com/research/"
    categories = {"íˆ¬ìì •ë³´": "invest_list.naver",
                  "ì¢…ëª©ë¶„ì„": "company_list.naver",
                  "ì‚°ì—…ë¶„ì„": "industry_list.naver",
                  "ê²½ì œë¶„ì„": "economy_list.naver"}
    reports = []
    for cat, path in categories.items():
        try:
            res = requests.get(base_url + path, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")
            for row in soup.select("table.type_1 tbody tr"):
                cols = row.find_all("td")
                if len(cols) < 4: continue
                date = cols[3].get_text(strip=True)
                if date != today_display: continue
                title_tag = cols[1].find("a")
                if not title_tag: continue
                detail_url = "https://finance.naver.com" + title_tag["href"]
                company = cols[2].get_text(strip=True)
                pdf_url = None
                try:
                    d_soup = BeautifulSoup(requests.get(detail_url, headers=HEADERS).text, "html.parser")
                    pdf_btn = d_soup.find("a", string=re.compile("ë¦¬í¬íŠ¸ë³´ê¸°"))
                    if pdf_btn: pdf_url = "https://finance.naver.com" + pdf_btn["href"]
                except: pass
                reports.append({"source": "ë„¤ì´ë²„", "category": cat, "title": title_tag.get_text(strip=True),
                                "company": company, "date": date, "url": detail_url, "pdf_url": pdf_url})
            time.sleep(1)
        except Exception as e:
            print(f"âš ï¸ ë„¤ì´ë²„ {cat} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    return reports

@tool("Hankyung Consensus Scraper")
def hankyung_scraper() -> List[Dict[str, str]]:
    url = "https://consensus.hankyung.com/analysis/list"
    reports = []
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        for row in soup.select("table tbody tr"):
            cols = row.find_all("td")
            if len(cols) < 4: continue
            date = cols[3].get_text(strip=True).replace("-", ".")
            if date != today_display: continue
            title_tag = cols[0].find("a")
            pdf_tag = row.find("a", href=re.compile(r"\.pdf$"))
            pdf_url = "https://consensus.hankyung.com" + pdf_tag["href"] if pdf_tag else None
            reports.append({"source": "í•œê²½ì»¨ì„¼ì„œìŠ¤", "category": cols[2].get_text(strip=True),
                            "title": title_tag.get_text(strip=True), "company": cols[1].get_text(strip=True),
                            "date": date, "url": "https://consensus.hankyung.com" + title_tag["href"],
                            "pdf_url": pdf_url})
            time.sleep(1)
    except Exception as e:
        print(f"âš ï¸ í•œê²½ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    return reports

# ----------------------------------------------------------
# 2ï¸âƒ£ Python Analyzer Tool
# ----------------------------------------------------------
class PythonAnalyzerTool(BaseTool):
    name = "Python Analyzer Tool"
    description = "ë¦¬í¬íŠ¸ ì œëª© ê¸°ë°˜ í‚¤ì›Œë“œ/ì¹´í…Œê³ ë¦¬ ë¶„ì„"
    def _run(self, reports: List[Dict[str, str]]) -> Dict[str, Any]:
        df = pd.DataFrame(reports).drop_duplicates(subset=["title", "company"])
        words = sum([re.findall(r"[ê°€-í£A-Za-z0-9]{2,12}", t) for t in df["title"]], [])
        stop = {"ë¦¬í¬íŠ¸", "ë¶„ì„", "ì „ë§", "íˆ¬ì", "ê²½ì œ", "ì‚°ì—…", "ì´ìŠˆ"}
        counter = Counter([w for w in words if w not in stop])
        return {
            "top_keywords": ", ".join([f"{k}({v}íšŒ)" for k, v in counter.most_common(7)]),
            "category_summary": df["category"].value_counts().to_dict()
        }

# ----------------------------------------------------------
# 3ï¸âƒ£ Report Summarizer Tool
# ----------------------------------------------------------
class ReportSummarizerTool(BaseTool):
    name = "Report Summarizer Tool"
    description = "PDF ë¦¬í¬íŠ¸ 2~3ì¤„ ìš”ì•½"
    def _run(self, reports: List[Dict[str, str]]) -> List[Dict[str, str]]:
        summaries = []
        for r in reports:
            title, company, category, pdf_url = r["title"], r["company"], r["category"], r.get("pdf_url")
            text = ""
            if pdf_url:
                try:
                    res = requests.get(pdf_url, headers=HEADERS, timeout=15)
                    with open("temp.pdf", "wb") as f: f.write(res.content)
                    with fitz.open("temp.pdf") as pdf:
                        for page in pdf: text += page.get_text()
                    os.remove("temp.pdf")
                except Exception as e: text = f"[PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}]"
            prompt = f"""
ë‹¹ì‹ ì€ 20ë…„ì°¨ ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
{company}ì˜ '{title}' ë¦¬í¬íŠ¸ë¥¼ 2~3ì¤„ë¡œ ìš”ì•½í•˜ì„¸ìš”.
ì‚¬ì‹¤ë§Œ ë‚¨ê¸°ê³  ì „ë§/ì˜ˆì¸¡ì€ ì œì™¸í•©ë‹ˆë‹¤.
ë³¸ë¬¸:
{text[:5000]}
"""
            try:
                resp = client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[{"role": "system", "content": "ì‚¬ì‹¤ ê¸°ë°˜ ìš”ì•½ë§Œ ìˆ˜í–‰."},
                              {"role": "user", "content": prompt}],
                    temperature=0.0)
                summary = resp.choices[0].message.content.strip()
            except Exception as e:
                summary = f"[ìš”ì•½ ì‹¤íŒ¨: {e}]"
            summaries.append({"category": category, "title": title,
                              "company": company, "summary": summary})
            time.sleep(1)
        return summaries

# ----------------------------------------------------------
# 4ï¸âƒ£ Final Briefing Tool (ë³µì›)
# ----------------------------------------------------------
class FinalBriefingTool(BaseTool):
    name = "Final Briefing Tool"
    description = "ë¶„ì„ ê²°ê³¼ì™€ ìš”ì•½ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¢…í•©í•´ ì¼ì¼ ë¸Œë¦¬í•‘ í…ìŠ¤íŠ¸ ì‘ì„±"
    def _run(self, summaries: List[Dict[str, str]], analysis: Dict[str, Any]) -> str:
        summary_texts = [f"[{s['category']}] {s['title']} â€” {s['summary']} ({s['company']})"
                         for s in summaries]
        prompt = f"""
[ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ]
{analysis.get("top_keywords", "N/A")}
[ì¹´í…Œê³ ë¦¬ë³„ ë¹„ì¤‘]
{analysis.get("category_summary", {})}

[ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸]
{chr(10).join(summary_texts)}

---
ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ íˆ¬ì ìŠ¤í„°ë””ìš© 'ì¼ì¼ ë¸Œë¦¬í•‘'ì„ ì‘ì„±í•˜ì‹œì˜¤.
1) í•µì‹¬ í…Œë§ˆ TOP 3~5
2) ê±°ì‹œê²½ì œ ìš”ì•½
3) ì£¼ìš” ì¢…ëª© ë° ì‚°ì—…ë³„ ìš”ì•½
"""
        resp = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "system", "content": "20ë…„ì°¨ ë¦¬ì„œì¹˜ ì• ë„ë¦¬ìŠ¤íŠ¸ë¡œ ì‚¬ì‹¤ ê¸°ë°˜ ë¸Œë¦¬í•‘ ì‘ì„±"},
                      {"role": "user", "content": prompt}],
            temperature=0.0)
        body = resp.choices[0].message.content.strip()
        header = f"# {today_file} ì¼ì¼ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë¸Œë¦¬í•‘\n\n*ì´ {len(summaries)}ê±´ ê¸°ë°˜ / {today_display} ë°œí–‰*\n\n"
        return header + body

# ----------------------------------------------------------
# 5ï¸âƒ£ Notion Upload Tool (ê°œì„ )
# ----------------------------------------------------------
class NotionUploadTool(BaseTool):
    name = "Notion Upload Tool"
    description = "ìµœì¢… ë¸Œë¦¬í•‘ê³¼ ë¶„ì„ê²°ê³¼ë¥¼ Notion DBì— ì—…ë¡œë“œ"
    def _run(self, briefing_text: str, analysis: Dict[str, Any]) -> str:
        page_data = {
            "parent": {"database_id": NOTION_DATABASE_ID},
            "properties": {
                "Date": {"date": {"start": today_file}},
                "Top Keywords": {"rich_text": [{"text": {"content": analysis.get("top_keywords", "")}}]},
                "Category Summary": {"rich_text": [{"text": {"content": str(analysis.get("category_summary", {}))}}]},
                "Name": {"title": [{"text": {"content": f"{today_file} ì¼ì¼ ë¸Œë¦¬í•‘"}}]}
            }
        }
        try:
            res = requests.post("https://api.notion.com/v1/pages", headers=NOTION_HEADERS, json=page_data)
            res.raise_for_status()
            parent_id = res.json()["id"]
        except Exception as e:
            return f"âš ï¸ ìƒìœ„ í˜ì´ì§€ ìƒì„± ì‹¤íŒ¨: {e}"

        children = []
        for i in range(0, len(briefing_text), 2000):
            children.append({"object": "block", "type": "paragraph",
                             "paragraph": {"rich_text": [{"type": "text", "text": {"content": briefing_text[i:i+2000]}}]}})
        try:
            res_blocks = requests.patch(f"https://api.notion.com/v1/blocks/{parent_id}/children",
                                        headers=NOTION_HEADERS, json={"children": children})
            res_blocks.raise_for_status()
        except Exception as e:
            return f"âš ï¸ í•˜ìœ„ ë¸”ë¡ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}"
        return f"âœ… Notion ì—…ë¡œë“œ ì™„ë£Œ (Page ID: {parent_id})"

# ----------------------------------------------------------
# 6ï¸âƒ£ CrewAI íŒŒì´í”„ë¼ì¸
# ----------------------------------------------------------
researcher = Agent(role="ë¦¬í¬íŠ¸ ìˆ˜ì§‘ê°€", tools=[naver_research_scraper, hankyung_scraper])
analyzer = Agent(role="ë¶„ì„ê°€", tools=[PythonAnalyzerTool()])
summarizer = Agent(role="ìš”ì•½ê°€", tools=[ReportSummarizerTool()])
analyst = Agent(role="ë¸Œë¦¬í•‘ ì‘ì„±ê°€", tools=[FinalBriefingTool()])
notion_uploader = Agent(role="Notion ì—…ë¡œë”", tools=[NotionUploadTool()])

task_collect = Task("ë¦¬í¬íŠ¸ ìˆ˜ì§‘", agent=researcher)
task_analyze = Task("í‚¤ì›Œë“œ ë¶„ì„", agent=analyzer, context=[task_collect])
task_summarize = Task("PDF ìš”ì•½", agent=summarizer, context=[task_collect])
task_briefing = Task("ì¼ì¼ ë¸Œë¦¬í•‘ ì‘ì„±", agent=analyst, context=[task_analyze, task_summarize])
task_upload = Task("Notion ì—…ë¡œë“œ", agent=notion_uploader, context=[task_briefing, task_analyze])

crew = Crew(agents=[researcher, analyzer, summarizer, analyst, notion_uploader],
            tasks=[task_collect, task_analyze, task_summarize, task_briefing, task_upload],
            process=Process.parallel, verbose=2)

if __name__ == "__main__":
    print(f"ğŸš€ {today_display} CrewAI Daily Briefing ì‹œì‘ (v7-Final)")
    result = crew.kickoff()
    print("âœ… ì™„ë£Œ:", result)
